"""
è‚¡ç¥¨å®ç›˜é¢„æµ‹æ¨¡å—
ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹å¯¹è¾“å…¥çš„è‚¡ç¥¨è¿›è¡Œé¢„æµ‹
"""

import pandas as pd
import numpy as np
import pickle
import os
import warnings
from datetime import datetime, timedelta
import sys

sys.path.append('.')

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')

# å¯¼å…¥æ•°æ®ä¸‹è½½æ¨¡å—
from stock_data_downloader import download_china_stock_enhanced_data

def load_trained_model(model_path='models/trained_model.pkl'):
    """
    åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
    
    å‚æ•°:
    model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    model: è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - {model_path}")
        print("è¯·å…ˆè¿è¡Œ stock_statistical_analysis.py è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹")
        return None
    
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
    return model

def load_all_models(models_path='models/all_trained_models.pkl'):
    """
    åŠ è½½æ‰€æœ‰è®­ç»ƒçš„æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”é¢„æµ‹ï¼‰
    
    å‚æ•°:
    models_path: æ‰€æœ‰æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    all_models_data: æ‰€æœ‰æ¨¡å‹çš„å­—å…¸
    """
    if not os.path.exists(models_path):
        print(f"âš ï¸  æœªæ‰¾åˆ°å¤šæ¨¡å‹æ–‡ä»¶: {models_path}")
        print("   å°†åªä½¿ç”¨å•ä¸€æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹")
        return None
    
    with open(models_path, 'rb') as f:
        all_models_data = pickle.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(all_models_data)} ä¸ªæ¨¡å‹: {list(all_models_data.keys())}")
    return all_models_data

def load_model_info(info_path='models/model_info.pkl'):
    """
    åŠ è½½æ¨¡å‹ä¿¡æ¯
    
    å‚æ•°:
    info_path: æ¨¡å‹ä¿¡æ¯æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    model_info: æ¨¡å‹ä¿¡æ¯å­—å…¸
    """
    if not os.path.exists(info_path):
        return None
    
    with open(info_path, 'rb') as f:
        model_info = pickle.load(f)
    
    return model_info

def load_feature_list(feature_path='models/feature_list.pkl'):
    """
    åŠ è½½ç‰¹å¾åˆ—è¡¨
    
    å‚æ•°:
    feature_path: ç‰¹å¾åˆ—è¡¨æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    feature_list: ç‰¹å¾åç§°åˆ—è¡¨
    """
    if not os.path.exists(feature_path):
        print(f"âŒ é”™è¯¯ï¼šç‰¹å¾åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ - {feature_path}")
        return None
    
    with open(feature_path, 'rb') as f:
        feature_list = pickle.load(f)
    
    # æ¸…ç†ç‰¹å¾åç§°ï¼ˆç¡®ä¿ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    cleaned_feature_list = []
    for col in feature_list:
        clean_col = str(col)
        clean_col = clean_col.replace('[', '_').replace(']', '_')
        clean_col = clean_col.replace('{', '_').replace('}', '_')
        clean_col = clean_col.replace('"', '').replace("'", '')
        clean_col = clean_col.replace(':', '_').replace(',', '_')
        clean_col = clean_col.replace(' ', '_').replace('<', 'lt')
        clean_col = clean_col.replace('>', 'gt').replace('=', 'eq')
        clean_col = clean_col.replace('(', '_').replace(')', '_')
        # å…³é”®ï¼šæŠŠæ‰€æœ‰åŒä¸‹åˆ’çº¿æ›¿æ¢æˆå•ä¸‹åˆ’çº¿
        while '__' in clean_col:
            clean_col = clean_col.replace('__', '_')
        clean_col = clean_col.strip('_')
        cleaned_feature_list.append(clean_col)
    
    print(f"âœ… æˆåŠŸåŠ è½½ç‰¹å¾åˆ—è¡¨: {len(cleaned_feature_list)} ä¸ªç‰¹å¾")
    return cleaned_feature_list

def download_stock_for_prediction(stock_code, days=365):
    """
    ä¸‹è½½è‚¡ç¥¨æœ€è¿‘çš„æ•°æ®ç”¨äºé¢„æµ‹
    
    å‚æ•°:
    stock_code: è‚¡ç¥¨ä»£ç 
    days: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®ï¼ˆé»˜è®¤365å¤©ï¼Œçº¦1å¹´ï¼‰
    
    è¿”å›:
    stock_data: è‚¡ç¥¨æ•°æ®DataFrame
    """
    print(f"\nğŸ“¥ ä¸‹è½½è‚¡ç¥¨ {stock_code} çš„æœ€æ–°æ•°æ®ï¼ˆæœ€è¿‘{days}å¤©ï¼‰...")
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    
    start_date_str = start_date.strftime('%Y%m%d')
    end_date_str = end_date.strftime('%Y%m%d')
    
    # ä¸‹è½½æ•°æ®
    all_data = download_china_stock_enhanced_data(
        [stock_code],
        start_date=start_date_str,
        end_date=end_date_str,
        save_to_file=False
    )
    
    if stock_code not in all_data:
        print(f"âŒ æ— æ³•ä¸‹è½½è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
        return None
    
    stock_data = all_data[stock_code]
    print(f"âœ… æˆåŠŸä¸‹è½½ {len(stock_data)} æ¡æ•°æ®è®°å½•")
    
    return stock_data

def extract_features_for_prediction(stock_data, window_size=20):
    """
    ä¸ºé¢„æµ‹æå–TSFreshç‰¹å¾
    
    å‚æ•°:
    stock_data: è‚¡ç¥¨æ•°æ®DataFrame
    window_size: æ»‘åŠ¨çª—å£å¤§å°
    
    è¿”å›:
    features_df: æå–çš„ç‰¹å¾DataFrame
    """
    print(f"\nğŸ”§ æå–æ—¶é—´åºåˆ—ç‰¹å¾ï¼ˆçª—å£å¤§å°: {window_size}å¤©ï¼‰...")
    
    from tsfresh import extract_features
    from tsfresh.feature_extraction import MinimalFCParameters
    from tsfresh.utilities.dataframe_functions import impute
    
    # æ£€æŸ¥æ•°æ®é•¿åº¦
    if len(stock_data) < window_size:
        print(f"âŒ æ•°æ®ä¸è¶³ï¼šéœ€è¦è‡³å°‘ {window_size} å¤©çš„æ•°æ®ï¼Œå½“å‰åªæœ‰ {len(stock_data)} å¤©")
        return None
    
    # é€‰æ‹©ç”¨äºTSFreshåˆ†æçš„ç‰¹å¾
    tsfresh_features = [
        'Close', 'Open', 'High', 'Low', 'Volume', 'TurnoverRate', 
        'PriceChangeRate', 'MainNetInflow', 'MainNetInflowRatio'
    ]
    
    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾å­˜åœ¨
    for feature in tsfresh_features:
        if feature not in stock_data.columns:
            stock_data[feature] = 0
    
    # ä½¿ç”¨æœ€è¿‘çš„çª—å£æ•°æ®
    window_data = stock_data.iloc[-window_size:]
    
    # åˆ›å»ºTSFreshæ ¼å¼çš„æ•°æ®
    tsfresh_data_list = []
    window_id = f"prediction_window"
    
    for feature in tsfresh_features:
        feature_values = window_data[feature].values
        
        for time_idx, value in enumerate(feature_values):
            tsfresh_data_list.append({
                'id': window_id,
                'time': time_idx,
                'feature_name': feature,
                'value': float(value) if not pd.isna(value) else 0.0
            })
    
    x_df = pd.DataFrame(tsfresh_data_list)
    
    # æå–ç‰¹å¾
    print("  æå–TSFreshç‰¹å¾...")
    feature_extraction_settings = MinimalFCParameters()
    
    all_extracted_features = []
    
    for feature_type in tsfresh_features:
        feature_df = x_df[x_df['feature_name'] == feature_type].copy()
        feature_df = feature_df.drop('feature_name', axis=1)
        
        try:
            extracted = extract_features(
                feature_df,
                column_id="id",
                column_sort="time",
                column_value="value",
                default_fc_parameters=feature_extraction_settings,
                n_jobs=1,
                disable_progressbar=True
            )
            
            # é‡å‘½ååˆ—
            extracted.columns = [f"{feature_type}_{col}" for col in extracted.columns]
            all_extracted_features.append(extracted)
            
        except Exception as e:
            print(f"  âš ï¸ {feature_type} ç‰¹å¾æå–å¤±è´¥: {e}")
            continue
    
    if not all_extracted_features:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        return None
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    x_extracted = pd.concat(all_extracted_features, axis=1)
    
    # å¤„ç†æ— é™å€¼å’Œç¼ºå¤±å€¼
    x_extracted = impute(x_extracted)
    
    # ç«‹å³æ¸…ç†ç‰¹å¾åç§°ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    x_extracted = clean_feature_names(x_extracted)
    
    print(f"âœ… æˆåŠŸæå– {x_extracted.shape[1]} ä¸ªç‰¹å¾")
    
    return x_extracted

def clean_feature_names(df):
    """æ¸…ç†ç‰¹å¾åç§°"""
    cleaned_columns = []
    for col in df.columns:
        clean_col = str(col)
        clean_col = clean_col.replace('[', '_').replace(']', '_')
        clean_col = clean_col.replace('{', '_').replace('}', '_')
        clean_col = clean_col.replace('"', '').replace("'", '')
        clean_col = clean_col.replace(':', '_').replace(',', '_')
        clean_col = clean_col.replace(' ', '_').replace('<', 'lt')
        clean_col = clean_col.replace('>', 'gt').replace('=', 'eq')
        clean_col = clean_col.replace('(', '_').replace(')', '_')
        while '__' in clean_col:
            clean_col = clean_col.replace('__', '_')
        clean_col = clean_col.strip('_')
        cleaned_columns.append(clean_col)
    
    df_cleaned = df.copy()
    df_cleaned.columns = cleaned_columns
    return df_cleaned

def align_features(features_df, feature_list):
    """
    å¯¹é½ç‰¹å¾ï¼ˆç¡®ä¿ä¸è®­ç»ƒæ—¶ç‰¹å¾ä¸€è‡´ï¼‰
    
    å‚æ•°:
    features_df: æå–çš„ç‰¹å¾DataFrameï¼ˆå·²æ¸…ç†ç‰¹å¾åç§°ï¼‰
    feature_list: è®­ç»ƒæ—¶çš„ç‰¹å¾åˆ—è¡¨ï¼ˆå·²æ¸…ç†ï¼‰
    
    è¿”å›:
    aligned_df: å¯¹é½åçš„ç‰¹å¾DataFrame
    """
    print(f"\nğŸ”„ å¯¹é½ç‰¹å¾...")
    
    # åˆ›å»ºä¸€ä¸ªç©ºDataFrameï¼ŒåŒ…å«æ‰€æœ‰è®­ç»ƒæ—¶çš„ç‰¹å¾
    aligned_df = pd.DataFrame(0, index=features_df.index, columns=feature_list)
    
    # å¡«å……å­˜åœ¨çš„ç‰¹å¾
    matched_count = 0
    matched_features = []
    for col in features_df.columns:
        if col in feature_list:
            aligned_df[col] = features_df[col]
            matched_count += 1
            matched_features.append(col)
    
    missing_in_prediction = [col for col in feature_list if col not in features_df.columns]
    missing_in_training = [col for col in features_df.columns if col not in feature_list]
    
    print(f"  è®­ç»ƒç‰¹å¾æ•°: {len(feature_list)}")
    print(f"  æå–ç‰¹å¾æ•°: {len(features_df.columns)}")
    print(f"  åŒ¹é…ç‰¹å¾æ•°: {matched_count}")
    print(f"  ç¼ºå¤±ç‰¹å¾æ•°: {len(missing_in_prediction)} (å°†ç”¨0å¡«å……)")
    
    if matched_count == 0:
        print(f"\nâš ï¸  è­¦å‘Šï¼šæ²¡æœ‰åŒ¹é…çš„ç‰¹å¾ï¼")
        print(f"  è®­ç»ƒç‰¹å¾ç¤ºä¾‹: {feature_list[:5]}")
        print(f"  æå–ç‰¹å¾ç¤ºä¾‹: {list(features_df.columns[:5])}")
    elif matched_count < len(feature_list) * 0.5:
        print(f"\nâš ï¸  è­¦å‘Šï¼šåŒ¹é…çš„ç‰¹å¾è¾ƒå°‘ï¼ˆ{matched_count}/{len(feature_list)}ï¼‰")
        print(f"  åŒ¹é…çš„ç‰¹å¾ç¤ºä¾‹: {matched_features[:3]}")
        if missing_in_prediction:
            print(f"  é¢„æµ‹æ—¶ç¼ºå¤±çš„ç‰¹å¾ç¤ºä¾‹: {missing_in_prediction[:3]}")
        if missing_in_training:
            print(f"  è®­ç»ƒæ—¶æ²¡æœ‰çš„ç‰¹å¾ç¤ºä¾‹: {missing_in_training[:3]}")
    else:
        print(f"  âœ… ç‰¹å¾åŒ¹é…è‰¯å¥½")
    
    print(f"âœ… ç‰¹å¾å¯¹é½å®Œæˆ")
    
    return aligned_df

def predict_stock(model, features_df):
    """
    ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
    
    å‚æ•°:
    model: è®­ç»ƒå¥½çš„æ¨¡å‹
    features_df: å¯¹é½åçš„ç‰¹å¾DataFrame
    
    è¿”å›:
    prediction: é¢„æµ‹ç±»åˆ« (0=å¼ºåŠ¿, 1=å¼±åŠ¿)
    probability: é¢„æµ‹æ¦‚ç‡
    """
    print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
    
    # é¢„æµ‹
    prediction = model.predict(features_df)[0]
    probability = model.predict_proba(features_df)[0]
    
    return prediction, probability

def predict_with_all_models(all_models_data, features_df):
    """
    ä½¿ç”¨æ‰€æœ‰æ¨¡å‹è¿›è¡Œé¢„æµ‹å¹¶å¯¹æ¯”
    
    å‚æ•°:
    all_models_data: æ‰€æœ‰æ¨¡å‹çš„å­—å…¸
    features_df: å¯¹é½åçš„ç‰¹å¾DataFrame
    
    è¿”å›:
    predictions_dict: æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
    """
    print(f"\nğŸ¤– ä½¿ç”¨ {len(all_models_data)} ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”é¢„æµ‹...")
    
    predictions_dict = {}
    
    for model_name, model_data in all_models_data.items():
        model = model_data['model']
        optimal_threshold = model_data.get('optimal_threshold', 0.5)
        
        # é¢„æµ‹æ¦‚ç‡
        probability = model.predict_proba(features_df)[0]
        
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„æœ€ä¼˜é˜ˆå€¼
        prediction = 1 if probability[1] >= optimal_threshold else 0
        
        predictions_dict[model_name] = {
            'prediction': prediction,
            'probability': probability,
            'prob_strong': probability[0],
            'prob_weak': probability[1],
            'confidence': max(probability),
            'optimal_threshold': optimal_threshold,
            'train_accuracy': model_data.get('accuracy', 0),
            'train_precision': model_data.get('avg_precision', 0)
        }
    
    return predictions_dict

def print_prediction_result(stock_code, stock_data, prediction, probability, model_info=None):
    """
    æ‰“å°é¢„æµ‹ç»“æœ
    
    å‚æ•°:
    stock_code: è‚¡ç¥¨ä»£ç 
    stock_data: è‚¡ç¥¨æ•°æ®
    prediction: é¢„æµ‹ç±»åˆ«
    probability: é¢„æµ‹æ¦‚ç‡
    model_info: æ¨¡å‹ä¿¡æ¯
    """
    print("\n" + "="*80)
    print(f"ğŸ“Š è‚¡ç¥¨é¢„æµ‹ç»“æœ - {stock_code}")
    print("="*80)
    
    # æœ€æ–°è¡Œæƒ…
    latest = stock_data.iloc[-1]
    latest_date = stock_data.index[-1].strftime('%Y-%m-%d')
    
    print(f"\nğŸ“… æœ€æ–°æ•°æ®æ—¥æœŸ: {latest_date}")
    print(f"ğŸ’° æœ€æ–°æ”¶ç›˜ä»·: {latest['Close']:.2f} å…ƒ")
    
    if 'MA_20' in stock_data.columns:
        ma20 = latest['MA_20']
    elif 'SMA_20' in stock_data.columns:
        ma20 = latest['SMA_20']
    else:
        ma20 = stock_data['Close'].tail(20).mean()
    
    print(f"ğŸ“ˆ 20æ—¥å‡çº¿(MA20): {ma20:.2f} å…ƒ")
    
    # å½“å‰çŠ¶æ€
    current_position = "å¼ºåŠ¿ (ä»·æ ¼â‰¥MA20)" if latest['Close'] >= ma20 else "å¼±åŠ¿ (ä»·æ ¼<MA20)"
    position_diff = ((latest['Close'] - ma20) / ma20) * 100
    print(f"ğŸ“ å½“å‰çŠ¶æ€: {current_position} ({position_diff:+.2f}%)")
    
    # é¢„æµ‹ç»“æœ
    print(f"\n" + "-"*80)
    print("ğŸ”® æ¨¡å‹é¢„æµ‹ï¼ˆæœªæ¥5æ—¥ï¼‰")
    print("-"*80)
    
    prediction_label = "å¼ºåŠ¿ (ä»·æ ¼â‰¥MA20)" if prediction == 0 else "å¼±åŠ¿ (ä»·æ ¼<MA20)"
    prob_strong = probability[0]  # å¼ºåŠ¿æ¦‚ç‡
    prob_weak = probability[1]    # å¼±åŠ¿æ¦‚ç‡
    
    if prediction == 0:
        print(f"âœ… é¢„æµ‹ç»“æœ: {prediction_label}")
        print(f"ğŸ“Š é¢„æµ‹ç½®ä¿¡åº¦: {prob_strong:.1%}")
        print(f"   - å¼ºåŠ¿æ¦‚ç‡: {prob_strong:.1%} â­")
        print(f"   - å¼±åŠ¿æ¦‚ç‡: {prob_weak:.1%}")
    else:
        print(f"âš ï¸  é¢„æµ‹ç»“æœ: {prediction_label}")
        print(f"ğŸ“Š é¢„æµ‹ç½®ä¿¡åº¦: {prob_weak:.1%}")
        print(f"   - å¼ºåŠ¿æ¦‚ç‡: {prob_strong:.1%}")
        print(f"   - å¼±åŠ¿æ¦‚ç‡: {prob_weak:.1%} â­")
    
    # ç½®ä¿¡åº¦è¯„çº§
    confidence = max(prob_strong, prob_weak)
    if confidence >= 0.80:
        confidence_level = "éå¸¸é«˜ â­â­â­â­â­"
    elif confidence >= 0.70:
        confidence_level = "é«˜ â­â­â­â­"
    elif confidence >= 0.60:
        confidence_level = "ä¸­ç­‰ â­â­â­"
    else:
        confidence_level = "è¾ƒä½ â­â­"
    
    print(f"\nğŸ¯ ç½®ä¿¡åº¦è¯„çº§: {confidence_level}")
    
    # æ¨¡å‹ä¿¡æ¯
    if model_info:
        print(f"\n" + "-"*80)
        print("ğŸ“ˆ æ¨¡å‹ä¿¡æ¯")
        print("-"*80)
        print(f"æ¨¡å‹ç±»å‹: {model_info.get('model_name', 'Unknown')}")
        print(f"è®­ç»ƒæ—¶é—´: {model_info.get('train_date', 'Unknown')}")
        print(f"æ¨¡å‹å‡†ç¡®ç‡: {model_info.get('accuracy', 0):.2%}")
        print(f"æ¨¡å‹ç²¾ç¡®ç‡: {model_info.get('avg_precision', 0):.2%}")
    
    # æ“ä½œå»ºè®®
    print(f"\n" + "-"*80)
    print("ğŸ’¡ æ“ä½œå»ºè®®")
    print("-"*80)
    
    if prediction == 0:  # é¢„æµ‹å¼ºåŠ¿
        if prob_strong >= 0.75:
            print("ğŸ“ˆ å»ºè®®: å¯è€ƒè™‘æŒæœ‰æˆ–é€‚å½“å¢ä»“")
            print("   ç†ç”±: æ¨¡å‹é¢„æµ‹å¼ºåŠ¿ä¸”ç½®ä¿¡åº¦è¾ƒé«˜")
        elif prob_strong >= 0.60:
            print("ğŸ“Š å»ºè®®: è§‚å¯Ÿä¸ºä¸»ï¼Œè°¨æ…æŒæœ‰")
            print("   ç†ç”±: æ¨¡å‹é¢„æµ‹å¼ºåŠ¿ä½†ç½®ä¿¡åº¦ä¸­ç­‰")
        else:
            print("âš ï¸  å»ºè®®: è°¨æ…å¯¹å¾…ï¼Œå»ºè®®è§‚æœ›")
            print("   ç†ç”±: é¢„æµ‹ç½®ä¿¡åº¦è¾ƒä½")
    else:  # é¢„æµ‹å¼±åŠ¿
        if prob_weak >= 0.75:
            print("ğŸ“‰ å»ºè®®: è€ƒè™‘å‡ä»“æˆ–è§‚æœ›")
            print("   ç†ç”±: æ¨¡å‹é¢„æµ‹å¼±åŠ¿ä¸”ç½®ä¿¡åº¦è¾ƒé«˜")
        elif prob_weak >= 0.60:
            print("âš ï¸  å»ºè®®: è°¨æ…æŒæœ‰ï¼Œæ§åˆ¶ä»“ä½")
            print("   ç†ç”±: æ¨¡å‹é¢„æµ‹å¼±åŠ¿ä½†ç½®ä¿¡åº¦ä¸­ç­‰")
        else:
            print("ğŸ“Š å»ºè®®: ç»§ç»­è§‚å¯Ÿï¼Œæš‚ä¸æ“ä½œ")
            print("   ç†ç”±: é¢„æµ‹ç½®ä¿¡åº¦è¾ƒä½")
    
    print(f"\n" + "="*80)
    print("âš ï¸  é£é™©æç¤º:")
    print("   1. æ¨¡å‹é¢„æµ‹ä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®")
    print("   2. è‚¡ç¥¨æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…")
    print("   3. è¯·ç»“åˆåŸºæœ¬é¢ã€æŠ€æœ¯é¢ç­‰å¤šæ–¹é¢å› ç´ ç»¼åˆåˆ¤æ–­")
    print("   4. å»ºè®®è®¾ç½®æ­¢æŸä½ï¼Œæ§åˆ¶é£é™©æ•å£")
    print("="*80 + "\n")

def print_models_comparison(stock_code, stock_data, predictions_dict, model_info=None):
    """
    æ‰“å°å¤šæ¨¡å‹å¯¹æ¯”ç»“æœ
    
    å‚æ•°:
    stock_code: è‚¡ç¥¨ä»£ç 
    stock_data: è‚¡ç¥¨æ•°æ®
    predictions_dict: æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœå­—å…¸
    model_info: æ¨¡å‹ä¿¡æ¯
    """
    print("\n" + "="*80)
    print(f"ğŸ“Š å¤šæ¨¡å‹é¢„æµ‹å¯¹æ¯” - {stock_code}")
    print("="*80)
    
    # æœ€æ–°è¡Œæƒ…
    latest = stock_data.iloc[-1]
    latest_date = stock_data.index[-1].strftime('%Y-%m-%d')
    
    print(f"\nğŸ“… æœ€æ–°æ•°æ®æ—¥æœŸ: {latest_date}")
    print(f"ğŸ’° æœ€æ–°æ”¶ç›˜ä»·: {latest['Close']:.2f} å…ƒ")
    
    if 'MA_20' in stock_data.columns:
        ma20 = latest['MA_20']
    elif 'SMA_20' in stock_data.columns:
        ma20 = latest['SMA_20']
    else:
        ma20 = stock_data['Close'].tail(20).mean()
    
    print(f"ğŸ“ˆ 20æ—¥å‡çº¿(MA20): {ma20:.2f} å…ƒ")
    
    # å½“å‰çŠ¶æ€
    current_position = "å¼ºåŠ¿ (ä»·æ ¼â‰¥MA20)" if latest['Close'] >= ma20 else "å¼±åŠ¿ (ä»·æ ¼<MA20)"
    position_diff = ((latest['Close'] - ma20) / ma20) * 100
    print(f"ğŸ“ å½“å‰çŠ¶æ€: {current_position} ({position_diff:+.2f}%)")
    
    # æ¨¡å‹å¯¹æ¯”
    print(f"\n" + "-"*80)
    print("ğŸ”® å¤šæ¨¡å‹é¢„æµ‹å¯¹æ¯”ï¼ˆæœªæ¥5æ—¥ï¼‰")
    print("-"*80)
    
    # ç»Ÿè®¡æŠ•ç¥¨ç»“æœ
    vote_strong = sum(1 for pred in predictions_dict.values() if pred['prediction'] == 0)
    vote_weak = sum(1 for pred in predictions_dict.values() if pred['prediction'] == 1)
    total_models = len(predictions_dict)
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    sorted_models = sorted(
        predictions_dict.items(), 
        key=lambda x: x[1]['confidence'], 
        reverse=True
    )
    
    # è¡¨å¤´
    print(f"\n{'æ¨¡å‹åç§°':<20} {'é¢„æµ‹':<10} {'å¼ºåŠ¿æ¦‚ç‡':<10} {'å¼±åŠ¿æ¦‚ç‡':<10} {'ç½®ä¿¡åº¦':<8} {'è®­ç»ƒç²¾ç¡®ç‡':<10}")
    print("-" * 80)
    
    for model_name, pred in sorted_models:
        prediction_label = "å¼ºåŠ¿â­" if pred['prediction'] == 0 else "å¼±åŠ¿âš ï¸"
        prob_strong = pred['prob_strong']
        prob_weak = pred['prob_weak']
        confidence = pred['confidence']
        train_precision = pred['train_precision']
        
        print(f"{model_name:<20} {prediction_label:<10} {prob_strong:>7.1%}   {prob_weak:>7.1%}   {confidence:>6.1%}  {train_precision:>8.1%}")
    
    # ç»¼åˆç»“è®º
    print(f"\n" + "-"*80)
    print("ğŸ“Š ç»¼åˆç»“è®º")
    print("-"*80)
    
    print(f"\næŠ•ç¥¨ç»Ÿè®¡:")
    print(f"  é¢„æµ‹å¼ºåŠ¿: {vote_strong}/{total_models} ä¸ªæ¨¡å‹ ({vote_strong/total_models:.1%})")
    print(f"  é¢„æµ‹å¼±åŠ¿: {vote_weak}/{total_models} ä¸ªæ¨¡å‹ ({vote_weak/total_models:.1%})")
    
    # åˆ¤æ–­ä¸€è‡´æ€§
    consensus = max(vote_strong, vote_weak) / total_models
    
    if consensus >= 0.8:
        consensus_level = "éå¸¸ä¸€è‡´ â­â­â­â­â­"
        consensus_desc = "æ‰€æœ‰æ¨¡å‹æ„è§é«˜åº¦ä¸€è‡´"
    elif consensus >= 0.6:
        consensus_level = "æ¯”è¾ƒä¸€è‡´ â­â­â­â­"
        consensus_desc = "å¤§å¤šæ•°æ¨¡å‹æ„è§ä¸€è‡´"
    else:
        consensus_level = "æ„è§åˆ†æ­§ â­â­"
        consensus_desc = "æ¨¡å‹æ„è§å­˜åœ¨åˆ†æ­§ï¼Œå»ºè®®è°¨æ…"
    
    print(f"\nä¸€è‡´æ€§è¯„çº§: {consensus_level}")
    print(f"è¯´æ˜: {consensus_desc}")
    
    # æœ€ç»ˆå»ºè®®
    print(f"\n" + "-"*80)
    print("ğŸ’¡ ç»¼åˆå»ºè®®")
    print("-"*80)
    
    if vote_strong > vote_weak:
        print(f"âœ… å¤šæ•°æ¨¡å‹é¢„æµ‹: å¼ºåŠ¿")
        if consensus >= 0.8:
            print("ğŸ“ˆ å»ºè®®: å¯è€ƒè™‘æŒæœ‰æˆ–é€‚å½“å¢ä»“")
            print("   ç†ç”±: æ¨¡å‹ä¸€è‡´æ€§é«˜ï¼Œé¢„æµ‹å¼ºåŠ¿")
        elif consensus >= 0.6:
            print("ğŸ“Š å»ºè®®: è§‚å¯Ÿä¸ºä¸»ï¼Œè°¨æ…æŒæœ‰")
            print("   ç†ç”±: å¤šæ•°æ¨¡å‹é¢„æµ‹å¼ºåŠ¿ï¼Œä½†å­˜åœ¨ä¸€å®šåˆ†æ­§")
        else:
            print("âš ï¸  å»ºè®®: è°¨æ…å¯¹å¾…ï¼Œå»ºè®®è§‚æœ›")
            print("   ç†ç”±: æ¨¡å‹æ„è§åˆ†æ­§è¾ƒå¤§")
    else:
        print(f"âš ï¸  å¤šæ•°æ¨¡å‹é¢„æµ‹: å¼±åŠ¿")
        if consensus >= 0.8:
            print("ğŸ“‰ å»ºè®®: è€ƒè™‘å‡ä»“æˆ–è§‚æœ›")
            print("   ç†ç”±: æ¨¡å‹ä¸€è‡´æ€§é«˜ï¼Œé¢„æµ‹å¼±åŠ¿")
        elif consensus >= 0.6:
            print("âš ï¸  å»ºè®®: è°¨æ…æŒæœ‰ï¼Œæ§åˆ¶ä»“ä½")
            print("   ç†ç”±: å¤šæ•°æ¨¡å‹é¢„æµ‹å¼±åŠ¿ï¼Œä½†å­˜åœ¨ä¸€å®šåˆ†æ­§")
        else:
            print("ğŸ“Š å»ºè®®: ç»§ç»­è§‚å¯Ÿï¼Œæš‚ä¸æ“ä½œ")
            print("   ç†ç”±: æ¨¡å‹æ„è§åˆ†æ­§è¾ƒå¤§")
    
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
    avg_confidence = sum(pred['confidence'] for pred in predictions_dict.values()) / total_models
    print(f"\nå¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.1%}")
    
    if avg_confidence >= 0.75:
        print("   è¯„çº§: é«˜ç½®ä¿¡åº¦ â­â­â­â­â­")
    elif avg_confidence >= 0.65:
        print("   è¯„çº§: ä¸­ç­‰ç½®ä¿¡åº¦ â­â­â­â­")
    elif avg_confidence >= 0.55:
        print("   è¯„çº§: è¾ƒä½ç½®ä¿¡åº¦ â­â­â­")
    else:
        print("   è¯„çº§: ä½ç½®ä¿¡åº¦ â­â­")
    
    # æ¨¡å‹ä¿¡æ¯
    if model_info:
        print(f"\n" + "-"*80)
        print("ğŸ“ˆ æ¨¡å‹è®­ç»ƒä¿¡æ¯")
        print("-"*80)
        print(f"è®­ç»ƒæ—¶é—´: {model_info.get('train_date', 'Unknown')}")
        print(f"æœ€ä½³æ¨¡å‹: {model_info.get('best_model_name', 'Unknown')}")
        print(f"æ¨¡å‹æ•°é‡: {len(predictions_dict)} ä¸ª")
    
    # é£é™©æç¤º
    print(f"\n" + "="*80)
    print("âš ï¸  é£é™©æç¤º:")
    print("   1. å¤šæ¨¡å‹é¢„æµ‹æä¾›æ›´å…¨é¢çš„å‚è€ƒï¼Œä½†ä¸ä¿è¯å‡†ç¡®")
    print("   2. å½“æ¨¡å‹æ„è§åˆ†æ­§æ—¶ï¼Œè¯´æ˜å¸‚åœºå¤„äºä¸ç¡®å®šçŠ¶æ€")
    print("   3. å»ºè®®ç»“åˆåŸºæœ¬é¢ã€æŠ€æœ¯é¢ç­‰å¤šæ–¹é¢å› ç´ ç»¼åˆåˆ¤æ–­")
    print("   4. è‚¡ç¥¨æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…")
    print("="*80 + "\n")

def predict_single_stock(stock_code, window_size=20, use_multi_models=True):
    """
    é¢„æµ‹å•åªè‚¡ç¥¨
    
    å‚æ•°:
    stock_code: è‚¡ç¥¨ä»£ç 
    window_size: çª—å£å¤§å°
    use_multi_models: æ˜¯å¦ä½¿ç”¨å¤šæ¨¡å‹å¯¹æ¯”ï¼ˆé»˜è®¤Trueï¼‰
    
    è¿”å›:
    success: æ˜¯å¦æˆåŠŸ
    """
    print("\n" + "="*80)
    print(f"ğŸ” å¼€å§‹é¢„æµ‹è‚¡ç¥¨: {stock_code}")
    print("="*80)
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        print("\næ­¥éª¤1: åŠ è½½æ¨¡å‹")
        print("-" * 50)
        
        # å°è¯•åŠ è½½å¤šæ¨¡å‹
        all_models_data = None
        if use_multi_models:
            all_models_data = load_all_models()
        
        # å¦‚æœå¤šæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å•ä¸€æ¨¡å‹
        if all_models_data is None:
            model = load_trained_model()
            if model is None:
                return False
        
        feature_list = load_feature_list()
        if feature_list is None:
            return False
        
        model_info = load_model_info()
        
        # 2. ä¸‹è½½æ•°æ®
        print("\næ­¥éª¤2: ä¸‹è½½æœ€æ–°æ•°æ®")
        print("-" * 50)
        stock_data = download_stock_for_prediction(stock_code, days=365)
        if stock_data is None:
            return False
        
        # 3. æå–ç‰¹å¾
        print("\næ­¥éª¤3: æå–ç‰¹å¾")
        print("-" * 50)
        features_df = extract_features_for_prediction(stock_data, window_size=window_size)
        if features_df is None:
            return False
        
        # 4. å¯¹é½ç‰¹å¾
        print("\næ­¥éª¤4: å¯¹é½ç‰¹å¾")
        print("-" * 50)
        aligned_features = align_features(features_df, feature_list)
        
        # 5. é¢„æµ‹
        print("\næ­¥éª¤5: æ‰§è¡Œé¢„æµ‹")
        print("-" * 50)
        
        # å¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼Œä½¿ç”¨å¤šæ¨¡å‹å¯¹æ¯”
        if all_models_data is not None and len(all_models_data) > 1:
            predictions_dict = predict_with_all_models(all_models_data, aligned_features)
            # 6. è¾“å‡ºå¤šæ¨¡å‹å¯¹æ¯”ç»“æœ
            print_models_comparison(stock_code, stock_data, predictions_dict, model_info)
        else:
            # ä½¿ç”¨å•ä¸€æ¨¡å‹é¢„æµ‹
            prediction, probability = predict_stock(model, aligned_features)
            # 6. è¾“å‡ºå•æ¨¡å‹ç»“æœ
            print_prediction_result(stock_code, stock_data, prediction, probability, model_info)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def predict_multiple_stocks(stock_codes, window_size=20):
    """
    æ‰¹é‡é¢„æµ‹å¤šåªè‚¡ç¥¨
    
    å‚æ•°:
    stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
    window_size: çª—å£å¤§å°
    """
    print("\n" + "="*80)
    print(f"ğŸ“Š æ‰¹é‡é¢„æµ‹ {len(stock_codes)} åªè‚¡ç¥¨")
    print("="*80)
    
    results = []
    
    for i, stock_code in enumerate(stock_codes, 1):
        print(f"\nè¿›åº¦: {i}/{len(stock_codes)}")
        success = predict_single_stock(stock_code, window_size)
        results.append({'stock_code': stock_code, 'success': success})
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        if i < len(stock_codes):
            import time
            time.sleep(1)
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š æ‰¹é‡é¢„æµ‹æ±‡æ€»")
    print("="*80)
    success_count = sum(1 for r in results if r['success'])
    print(f"æˆåŠŸé¢„æµ‹: {success_count}/{len(stock_codes)}")
    
    failed = [r['stock_code'] for r in results if not r['success']]
    if failed:
        print(f"å¤±è´¥è‚¡ç¥¨: {', '.join(failed)}")

def main():
    """
    ä¸»å‡½æ•°ï¼šå®ç›˜é¢„æµ‹
    """
    print("="*80)
    print("ğŸ”® è‚¡ç¥¨å®ç›˜é¢„æµ‹ç³»ç»Ÿ")
    print("="*80)
    print("åŸºäºå·²è®­ç»ƒçš„æ¨¡å‹å¯¹è¾“å…¥è‚¡ç¥¨è¿›è¡Œé¢„æµ‹")
    print("é¢„æµ‹ç›®æ ‡ï¼šæœªæ¥5æ—¥ä»·æ ¼ç›¸å¯¹äº20æ—¥å‡çº¿çš„ä½ç½®")
    print("="*80)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists('models/trained_model.pkl'):
        print("\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
        print("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤è®­ç»ƒæ¨¡å‹:")
        print("  1. python stock_data_downloader.py")
        print("  2. python stock_feature_engineering.py")
        print("  3. python stock_statistical_analysis.py")
        return
    
    print("\nè¯·é€‰æ‹©é¢„æµ‹æ¨¡å¼:")
    print("  1. å•åªè‚¡ç¥¨é¢„æµ‹")
    print("  2. æ‰¹é‡è‚¡ç¥¨é¢„æµ‹")
    print("  3. ä½¿ç”¨é»˜è®¤æµ‹è¯•è‚¡ç¥¨")
    
    try:
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3): ").strip()
        
        if choice == '1':
            # å•åªè‚¡ç¥¨é¢„æµ‹
            stock_code = input("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç ï¼ˆ6ä½æ•°å­—ï¼‰: ").strip()
            if len(stock_code) != 6 or not stock_code.isdigit():
                print("âŒ é”™è¯¯ï¼šè‚¡ç¥¨ä»£ç æ ¼å¼ä¸æ­£ç¡®")
                return
            
            predict_single_stock(stock_code, window_size=20)
            
        elif choice == '2':
            # æ‰¹é‡é¢„æµ‹
            print("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç ï¼ˆå¤šä¸ªä»£ç ç”¨é€—å·æˆ–ç©ºæ ¼åˆ†éš”ï¼‰:")
            codes_input = input().strip()
            
            # è§£æè‚¡ç¥¨ä»£ç 
            stock_codes = []
            for code in codes_input.replace(',', ' ').split():
                code = code.strip()
                if len(code) == 6 and code.isdigit():
                    stock_codes.append(code)
            
            if not stock_codes:
                print("âŒ é”™è¯¯ï¼šæœªè¾“å…¥æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç ")
                return
            
            predict_multiple_stocks(stock_codes, window_size=20)
            
        elif choice == '3':
            # ä½¿ç”¨é»˜è®¤æµ‹è¯•è‚¡ç¥¨
            test_stocks = [
                '600519',  # è´µå·èŒ…å°
                '000001',  # å¹³å®‰é“¶è¡Œ
                '600036',  # æ‹›å•†é“¶è¡Œ
            ]
            
            print(f"\nä½¿ç”¨é»˜è®¤æµ‹è¯•è‚¡ç¥¨: {', '.join(test_stocks)}")
            predict_multiple_stocks(test_stocks, window_size=20)
            
        else:
            print("âŒ é”™è¯¯ï¼šæ— æ•ˆçš„é€‰é¡¹")
            return
        
        print("\nâœ… é¢„æµ‹å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()

