"""
è‚¡ç¥¨ç»Ÿè®¡åˆ†ææ¨¡å—
è´Ÿè´£æ¨¡å‹è®­ç»ƒã€é˜ˆå€¼ä¼˜åŒ–ã€ç»“æœå¯è§†åŒ–ç­‰ç»Ÿè®¡åˆ†æå·¥ä½œ
"""

import pandas as pd
import numpy as np
import os
import multiprocessing as mp
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, accuracy_score, precision_score, 
    recall_score, f1_score, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# ä»ç‰¹å¾å·¥ç¨‹æ¨¡å—å¯¼å…¥æ•°æ®åŠ è½½å‡½æ•°
from stock_feature_engineering import load_processed_features

# æŠ‘åˆ¶å„ç§è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')
warnings.filterwarnings('ignore', category=UserWarning, module='joblib')
warnings.filterwarnings('ignore', category=FutureWarning)
os.environ['LOKY_MAX_CPU_COUNT'] = str(mp.cpu_count())

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

def find_optimal_threshold(y_true, y_proba, metric='precision', min_recall=0.3):
    """
    è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜åˆ†ç±»é˜ˆå€¼ï¼ˆV2.4.2æ”¹è¿›ç‰ˆï¼‰
    
    å‚æ•°:
    y_true: çœŸå®æ ‡ç­¾
    y_proba: é¢„æµ‹æ¦‚ç‡ï¼ˆç±»åˆ«1çš„æ¦‚ç‡ï¼‰
    metric: ä¼˜åŒ–æŒ‡æ ‡ ('precision', 'f1', 'balanced')
    min_recall: æœ€å°å¬å›ç‡è¦æ±‚ï¼ˆé˜²æ­¢æç«¯é˜ˆå€¼ï¼‰
    
    è¿”å›:
    optimal_threshold: æœ€ä¼˜é˜ˆå€¼
    best_score: æœ€ä¼˜å¾—åˆ†
    results_df: æ‰€æœ‰é˜ˆå€¼çš„è¯„ä¼°ç»“æœ
    """
    print(f"\nğŸ¯ è‡ªåŠ¨æœç´¢æœ€ä¼˜é˜ˆå€¼ï¼ˆä¼˜åŒ–æŒ‡æ ‡: {metric}, æœ€å°å¬å›ç‡è¦æ±‚: {min_recall:.0%}ï¼‰...")
    
    thresholds = np.arange(0.1, 0.95, 0.01)
    best_threshold = 0.5
    best_score = 0
    threshold_results = []
    valid_count = 0
    
    for threshold in thresholds:
        y_pred = (y_proba >= threshold).astype(int)
        
        # è®¡ç®—å„ç±»åˆ«æŒ‡æ ‡
        precision_0 = precision_score(y_true, y_pred, pos_label=0, zero_division=0)
        precision_1 = precision_score(y_true, y_pred, pos_label=1, zero_division=0)
        recall_0 = recall_score(y_true, y_pred, pos_label=0, zero_division=0)
        recall_1 = recall_score(y_true, y_pred, pos_label=1, zero_division=0)
        
        # æ£€æŸ¥å¬å›ç‡çº¦æŸ
        min_class_recall = min(recall_0, recall_1)
        
        avg_precision = (precision_0 + precision_1) / 2
        avg_recall = (recall_0 + recall_1) / 2
        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
        balanced_score = (avg_precision + avg_recall) / 2
        
        threshold_results.append({
            'threshold': threshold,
            'precision': avg_precision,
            'f1': f1,
            'balanced': balanced_score,
            'precision_0': precision_0,
            'precision_1': precision_1,
            'recall_0': recall_0,
            'recall_1': recall_1,
            'min_recall': min_class_recall,
            'valid': min_class_recall >= min_recall
        })
        
        # åªè€ƒè™‘æ»¡è¶³æœ€å°å¬å›ç‡è¦æ±‚çš„é˜ˆå€¼
        if min_class_recall >= min_recall:
            valid_count += 1
            
            # æ ¹æ®æŒ‡å®šæŒ‡æ ‡é€‰æ‹©æœ€ä¼˜é˜ˆå€¼
            if metric == 'precision':
                score = avg_precision
            elif metric == 'f1':
                score = f1
            else:  # balanced
                score = balanced_score
            
            if score > best_score:
                best_score = score
                best_threshold = threshold
    
    # æ‰“å°é˜ˆå€¼æœç´¢ç»“æœ
    results_df = pd.DataFrame(threshold_results)
    print(f"  æ‰«æäº† {len(thresholds)} ä¸ªé˜ˆå€¼å€™é€‰")
    print(f"  æ»¡è¶³å¬å›ç‡è¦æ±‚çš„é˜ˆå€¼: {valid_count} ä¸ª")
    print(f"  æœ€ä¼˜é˜ˆå€¼: {best_threshold:.3f}")
    print(f"  æœ€ä¼˜{metric}å¾—åˆ†: {best_score:.4f}")
    
    # æ˜¾ç¤ºæœ€ä¼˜é˜ˆå€¼çš„è¯¦ç»†ä¿¡æ¯
    best_row = results_df[results_df['threshold'] == best_threshold].iloc[0]
    print(f"\n  æœ€ä¼˜é˜ˆå€¼è¯¦ç»†è¡¨ç°:")
    print(f"    å¼ºåŠ¿ç±»åˆ« - ç²¾ç¡®ç‡:{best_row['precision_0']:.1%}, å¬å›ç‡:{best_row['recall_0']:.1%}")
    print(f"    å¼±åŠ¿ç±»åˆ« - ç²¾ç¡®ç‡:{best_row['precision_1']:.1%}, å¬å›ç‡:{best_row['recall_1']:.1%}")
    print(f"    å¹³å‡ç²¾ç¡®ç‡: {best_row['precision']:.1%}")
    print(f"    F1åˆ†æ•°: {best_row['f1']:.3f}")
    
    # æ˜¾ç¤ºæœ€ä¼˜é˜ˆå€¼é™„è¿‘çš„ç»“æœ
    print(f"\n  å€™é€‰é˜ˆå€¼å¯¹æ¯”ï¼ˆå‰5ä¸ªï¼‰:")
    valid_df = results_df[results_df['valid'] == True].sort_values('precision', ascending=False)
    for idx, (_, row) in enumerate(valid_df.head(5).iterrows(), 1):
        print(f"    {idx}. é˜ˆå€¼={row['threshold']:.3f}: ç²¾ç¡®ç‡={row['precision']:.1%}, "
              f"F1={row['f1']:.3f}, å¬å›ç‡({row['recall_0']:.1%}/{row['recall_1']:.1%})")
    
    if valid_count == 0:
        print(f"\n  âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰é˜ˆå€¼æ»¡è¶³æœ€å°å¬å›ç‡{min_recall:.0%}è¦æ±‚ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼0.5")
        best_threshold = 0.5
    
    return best_threshold, best_score, results_df

def clean_feature_names(df):
    """
    æ¸…ç†ç‰¹å¾åç§°ï¼Œç§»é™¤LightGBM/XGBoostä¸æ”¯æŒçš„ç‰¹æ®Šå­—ç¬¦
    
    å‚æ•°:
    df: è¾“å…¥DataFrame
    
    è¿”å›:
    df_cleaned: æ¸…ç†åçš„DataFrame
    """
    cleaned_columns = []
    for col in df.columns:
        clean_col = str(col)
        clean_col = clean_col.replace('[', '_').replace(']', '_')
        clean_col = clean_col.replace('{', '_').replace('}', '_')
        clean_col = clean_col.replace('"', '').replace("'", '')
        clean_col = clean_col.replace(':', '_').replace(',', '_')
        clean_col = clean_col.replace(' ', '_').replace('<', 'lt')
        clean_col = clean_col.replace('>', 'gt').replace('=', 'eq')
        clean_col = clean_col.replace('(', '_').replace(')', '_')
        while '__' in clean_col:
            clean_col = clean_col.replace('__', '_')
        clean_col = clean_col.strip('_')
        cleaned_columns.append(clean_col)
    
    df_cleaned = df.copy()
    df_cleaned.columns = cleaned_columns
    return df_cleaned

def train_binary_model(X_train, X_test, y_train, y_test, use_smote=True, use_multi_models=False):
    """
    è®­ç»ƒäºŒåˆ†ç±»æ¨¡å‹ï¼ˆV2.4.2ä¼˜åŒ–ç‰ˆï¼‰
    
    å‚æ•°:
    X_train: è®­ç»ƒç‰¹å¾
    X_test: æµ‹è¯•ç‰¹å¾
    y_train: è®­ç»ƒæ ‡ç­¾
    y_test: æµ‹è¯•æ ‡ç­¾
    use_smote: æ˜¯å¦ä½¿ç”¨SMOTEè¿‡é‡‡æ ·
    use_multi_models: æ˜¯å¦å°è¯•å¤šç§ç®—æ³•ï¼ˆXGBoostã€LightGBMç­‰ï¼‰
    
    è¿”å›:
    best_model: æœ€ä½³æ¨¡å‹
    y_pred: é¢„æµ‹ç»“æœ
    accuracy: å‡†ç¡®ç‡
    model_name: æ¨¡å‹åç§°
    """
    print("è®­ç»ƒäºŒåˆ†ç±»æ¨¡å‹ï¼ˆV2.4.2ä¼˜åŒ–ç‰ˆï¼‰...")
    print("ğŸ¯ ä¼˜åŒ–ç›®æ ‡: æé«˜ç²¾ç¡®ç‡è‡³75%+ï¼Œç¡®ä¿å¬å›ç‡å¹³è¡¡")
    
    # æ‰“å°ç±»åˆ«åˆ†å¸ƒ
    print("\nè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
    train_counts = pd.Series(y_train).value_counts().sort_index()
    for label, count in train_counts.items():
        label_name = ['ä»·æ ¼>=MA20(å¼ºåŠ¿)', 'ä»·æ ¼<MA20(å¼±åŠ¿)'][label]
        print(f"  {label}: {label_name} - {count} æ ·æœ¬ ({count/len(y_train):.2%})")
    
    print("\næµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ:")
    test_counts = pd.Series(y_test).value_counts().sort_index()
    for label, count in test_counts.items():
        label_name = ['ä»·æ ¼>=MA20(å¼ºåŠ¿)', 'ä»·æ ¼<MA20(å¼±åŠ¿)'][label]
        print(f"  {label}: {label_name} - {count} æ ·æœ¬ ({count/len(y_test):.2%})")
    
    # æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡ç¨‹åº¦
    minority_ratio = train_counts.min() / train_counts.max()
    print(f"\nç±»åˆ«ä¸å¹³è¡¡æ¯”ç‡: {minority_ratio:.2%}")
    
    # SMOTEè¿‡é‡‡æ ·
    if use_smote:
        try:
            from imblearn.combine import SMOTETomek
            print("\nä½¿ç”¨SMOTEè¿‡é‡‡æ ·å¹³è¡¡æ•°æ®é›†...")
            smote_tomek = SMOTETomek(random_state=42)
            X_train_use, y_train_use = smote_tomek.fit_resample(X_train, y_train)
            print(f"åŸå§‹è®­ç»ƒé›†å¤§å°: {len(y_train)}")
            print(f"å¹³è¡¡åè®­ç»ƒé›†å¤§å°: {len(y_train_use)}")
            
            balanced_counts = pd.Series(y_train_use).value_counts().sort_index()
            print("å¹³è¡¡åç±»åˆ«åˆ†å¸ƒ:")
            for label, count in balanced_counts.items():
                label_name = ['ä»·æ ¼>=MA20(å¼ºåŠ¿)', 'ä»·æ ¼<MA20(å¼±åŠ¿)'][label]
                print(f"  {label}: {label_name} - {count} æ ·æœ¬ ({count/len(y_train_use):.2%})")
        except ImportError:
            print("\nâš ï¸ imbalanced-learnåº“æœªå®‰è£…ï¼Œè·³è¿‡SMOTE")
            print("   å®‰è£…å‘½ä»¤: pip install imbalanced-learn")
            X_train_use = X_train
            y_train_use = y_train
    else:
        X_train_use = X_train
        y_train_use = y_train
    
    # æ³¨æ„ï¼šç‰¹å¾åç§°å·²åœ¨mainå‡½æ•°ä¸­æ¸…ç†è¿‡ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
    X_train_cleaned = X_train_use
    X_test_cleaned = X_test
    print(f"\nç‰¹å¾æ•°é‡: {len(X_train_cleaned.columns)}")
    
    # è®­ç»ƒæ¨¡å‹
    models_to_try = {}
    
    # æ¨¡å‹1: Random Forestï¼ˆå¿…é€‰ï¼‰
    print("\n1ï¸âƒ£ è®­ç»ƒRandom Forestæ¨¡å‹...")
    rf_model = RandomForestClassifier(
        n_estimators=1000,
        random_state=42,
        max_depth=10,
        min_samples_split=20,
        min_samples_leaf=10,
        max_features='log2',
        class_weight={0: 1, 1: 2.5},
        criterion='gini',
        bootstrap=True,
        n_jobs=-1
    )
    rf_model.fit(X_train_cleaned, y_train_use)
    models_to_try['RandomForest'] = rf_model
    print("   âœ… Random Forestè®­ç»ƒå®Œæˆ")
    
    # å¯é€‰ï¼šå…¶ä»–ç®—æ³•
    if use_multi_models:
        # XGBoost
        try:
            import xgboost as xgb
            print("\n2ï¸âƒ£ è®­ç»ƒXGBoostæ¨¡å‹...")
            neg_count = (y_train_use == 0).sum()
            pos_count = (y_train_use == 1).sum()
            scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1
            
            xgb_model = xgb.XGBClassifier(
                n_estimators=500,
                max_depth=6,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                scale_pos_weight=scale_pos_weight,
                random_state=42,
                n_jobs=-1,
                eval_metric='logloss'
            )
            xgb_model.fit(X_train_cleaned, y_train_use)
            models_to_try['XGBoost'] = xgb_model
            print("   âœ… XGBoostè®­ç»ƒå®Œæˆ")
        except ImportError:
            print("   âš ï¸ XGBoostæœªå®‰è£…ï¼Œè·³è¿‡")
        except Exception as e:
            print(f"   âš ï¸ XGBoostè®­ç»ƒå¤±è´¥: {e}")
        
        # LightGBM
        try:
            import lightgbm as lgb
            print("\n3ï¸âƒ£ è®­ç»ƒLightGBMæ¨¡å‹...")
            lgb_model = lgb.LGBMClassifier(
                n_estimators=500,
                max_depth=6,
                learning_rate=0.05,
                num_leaves=31,
                subsample=0.8,
                colsample_bytree=0.8,
                class_weight={0: 1, 1: 2.5},
                random_state=42,
                n_jobs=-1,
                verbose=-1
            )
            lgb_model.fit(X_train_cleaned, y_train_use)
            models_to_try['LightGBM'] = lgb_model
            print("   âœ… LightGBMè®­ç»ƒå®Œæˆ")
        except ImportError:
            print("   âš ï¸ LightGBMæœªå®‰è£…ï¼Œè·³è¿‡")
        except Exception as e:
            print(f"   âš ï¸ LightGBMè®­ç»ƒå¤±è´¥: {e}")
    
    # è¯„ä¼°æ‰€æœ‰æ¨¡å‹ï¼Œé€‰æ‹©æœ€ä½³
    print(f"\nğŸ“Š è¯„ä¼° {len(models_to_try)} ä¸ªæ¨¡å‹...")
    best_model = None
    best_model_name = None
    best_precision = 0
    
    for model_name, model in models_to_try.items():
        y_proba_temp = model.predict_proba(X_test_cleaned)[:, 1]
        
        # ä½¿ç”¨è‡ªåŠ¨é˜ˆå€¼ä¼˜åŒ–
        optimal_threshold_temp, _, _ = find_optimal_threshold(
            y_test, y_proba_temp, metric='precision', min_recall=0.3
        )
        
        y_pred_temp = (y_proba_temp >= optimal_threshold_temp).astype(int)
        
        precision_0 = precision_score(y_test, y_pred_temp, pos_label=0, zero_division=0)
        precision_1 = precision_score(y_test, y_pred_temp, pos_label=1, zero_division=0)
        avg_precision = (precision_0 + precision_1) / 2
        
        print(f"  {model_name}: å¹³å‡ç²¾ç¡®ç‡={avg_precision:.2%} (é˜ˆå€¼={optimal_threshold_temp:.3f})")
        
        if avg_precision > best_precision:
            best_precision = avg_precision
            best_model = model
            best_model_name = model_name
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (ç²¾ç¡®ç‡={best_precision:.2%})")
    
    # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆé¢„æµ‹
    y_proba = best_model.predict_proba(X_test_cleaned)[:, 1]
    optimal_threshold, _, _ = find_optimal_threshold(
        y_test, y_proba, metric='precision', min_recall=0.3
    )
    
    y_pred = (y_proba >= optimal_threshold).astype(int)
    accuracy = accuracy_score(y_test, y_pred)
    
    # è®¡ç®—ç²¾ç¡®ç‡
    precision_0 = precision_score(y_test, y_pred, pos_label=0, zero_division=0)
    precision_1 = precision_score(y_test, y_pred, pos_label=1, zero_division=0)
    avg_precision = (precision_0 + precision_1) / 2
    
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°")
    print("=" * 70)
    print(f"æ•´ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"\nç²¾ç¡®ç‡å¯¹æ¯”:")
    print(f"  å¼ºåŠ¿ç±»åˆ«: {precision_0:.2%}")
    print(f"  å¼±åŠ¿ç±»åˆ«: {precision_1:.2%}")
    print(f"  å¹³å‡ç²¾ç¡®ç‡: {avg_precision:.2%}")
    
    if avg_precision >= 0.75:
        print(f"  âœ… ä¼˜ç§€ï¼ç²¾ç¡®ç‡å·²è¾¾åˆ°75%ä»¥ä¸Š")
    elif avg_precision >= 0.70:
        print(f"  âœ… è‰¯å¥½ï¼ç²¾ç¡®ç‡è¾¾åˆ°70%+")
    else:
        print(f"  âš ï¸ å¯ä»¥æ¥å—ï¼Œä½†ä»æœ‰æå‡ç©ºé—´")
    
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, 
                              target_names=['ä»·æ ¼>=MA20(å¼ºåŠ¿)', 'ä»·æ ¼<MA20(å¼±åŠ¿)'],
                              zero_division=0))
    
    return best_model, y_pred, accuracy, best_model_name, models_to_try

def visualize_binary_results(y_test, y_pred, accuracy, output_dir='results'):
    """
    å¯è§†åŒ–äºŒåˆ†ç±»ç»“æœ
    
    å‚æ•°:
    y_test: æµ‹è¯•é›†çœŸå®æ ‡ç­¾
    y_pred: é¢„æµ‹æ ‡ç­¾
    accuracy: å‡†ç¡®ç‡
    output_dir: è¾“å‡ºç›®å½•
    """
    print(f"\nç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    cm = confusion_matrix(y_test, y_pred)
    class_names = ['ä»·æ ¼>=MA20(å¼ºåŠ¿)', 'ä»·æ ¼<MA20(å¼±åŠ¿)']
    
    plt.figure(figsize=(15, 5))
    
    # å­å›¾1: æ··æ·†çŸ©é˜µ
    plt.subplot(1, 3, 1)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f'äºŒåˆ†ç±»æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {accuracy:.4f}')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    
    # å­å›¾2: ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”
    plt.subplot(1, 3, 2)
    test_dist = pd.Series(y_test).value_counts().sort_index()
    pred_dist = pd.Series(y_pred).value_counts().sort_index()
    
    x = np.arange(len(class_names))
    width = 0.35
    
    plt.bar(x - width/2, test_dist.values, width, label='çœŸå®åˆ†å¸ƒ', alpha=0.8)
    plt.bar(x + width/2, pred_dist.values, width, label='é¢„æµ‹åˆ†å¸ƒ', alpha=0.8)
    
    plt.xlabel('ç±»åˆ«')
    plt.ylabel('æ ·æœ¬æ•°é‡')
    plt.title('ç±»åˆ«åˆ†å¸ƒå¯¹æ¯”')
    plt.xticks(x, class_names, rotation=45)
    plt.legend()
    
    # å­å›¾3: åˆ†ç±»å‡†ç¡®ç‡
    plt.subplot(1, 3, 3)
    class_accuracy = []
    for i in range(len(class_names)):
        mask = y_test == i
        if mask.sum() > 0:
            acc = (y_pred[mask] == i).mean()
            class_accuracy.append(acc)
        else:
            class_accuracy.append(0)
    
    bars = plt.bar(class_names, class_accuracy, alpha=0.8, color=['lightblue', 'red'])
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title('å„ç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡')
    plt.xticks(rotation=45)
    
    for bar, acc in zip(bars, class_accuracy):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{acc:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    
    output_path = f'{output_dir}/binary_classification_results.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {output_path}")
    plt.show()

def print_detailed_summary(y_test, y_pred, accuracy, stock_count, feature_count):
    """
    æ‰“å°è¯¦ç»†çš„åˆ†ææ€»ç»“
    
    å‚æ•°:
    y_test: æµ‹è¯•é›†çœŸå®æ ‡ç­¾
    y_pred: é¢„æµ‹æ ‡ç­¾
    accuracy: å‡†ç¡®ç‡
    stock_count: è‚¡ç¥¨æ•°é‡
    feature_count: ç‰¹å¾æ•°é‡
    """
    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
    tn, fp, fn, tp = cm.ravel()
    
    print("\n" + "="*80)
    print("ğŸ“Š åˆ†æç»“æœè¯¦ç»†æ€»ç»“")
    print("="*80)
    
    print(f"\nğŸ¯ æ•´ä½“æƒ…å†µ")
    print("-" * 50)
    print(f"åˆ†æè‚¡ç¥¨æ•°é‡: {stock_count}")
    print(f"æå–ç‰¹å¾æ•°é‡: {feature_count}")
    print(f"ç”Ÿæˆæ ·æœ¬æ•°é‡: {len(y_test) + len(y_pred)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(y_test)}")
    
    print(f"\nğŸ“ˆ æ¨¡å‹æ€§èƒ½")
    print("-" * 50)
    print(f"æ•´ä½“å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy:.2%})")
    
    # è®¡ç®—å„ç±»åˆ«æŒ‡æ ‡
    precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0
    recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0
    f1_0 = 2 * precision_0 * recall_0 / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0
    
    precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_1 = 2 * precision_1 * recall_1 / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0
    
    print(f"\nå¼ºåŠ¿ç±»åˆ« (ä»·æ ¼â‰¥MA20):")
    print(f"  ç²¾ç¡®ç‡: {precision_0:.2%}")
    print(f"  å¬å›ç‡: {recall_0:.2%}")
    print(f"  F1åˆ†æ•°: {f1_0:.2%}")
    
    print(f"\nå¼±åŠ¿ç±»åˆ« (ä»·æ ¼<MA20):")
    print(f"  ç²¾ç¡®ç‡: {precision_1:.2%}")
    print(f"  å¬å›ç‡: {recall_1:.2%}")
    print(f"  F1åˆ†æ•°: {f1_1:.2%}")
    
    avg_precision = (precision_0 + precision_1) / 2
    print(f"\nå¹³å‡ç²¾ç¡®ç‡: {avg_precision:.2%}")
    
    if avg_precision >= 0.75:
        print("  âœ… ä¼˜ç§€ï¼ç²¾ç¡®ç‡å·²è¾¾åˆ°75%ä»¥ä¸Š")
    elif avg_precision >= 0.70:
        print("  âœ… è‰¯å¥½ï¼ç²¾ç¡®ç‡è¾¾åˆ°70%+")
    elif avg_precision >= 0.65:
        print("  âš ï¸ å¯ä»¥æ¥å—ï¼Œä½†ä»æœ‰æå‡ç©ºé—´")
    else:
        print("  âŒ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    print("\n" + "="*80)

def print_metrics_explanation(y_test, y_pred):
    """
    æ‰“å°è¯¦ç»†çš„æŒ‡æ ‡è§£é‡Šï¼ˆé€šä¿—æ˜“æ‡‚ç‰ˆï¼‰
    
    å‚æ•°:
    y_test: æµ‹è¯•é›†çœŸå®æ ‡ç­¾
    y_pred: é¢„æµ‹æ ‡ç­¾
    """
    cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
    tn, fp, fn, tp = cm.ravel()
    
    # è®¡ç®—å„ç±»åˆ«æŒ‡æ ‡
    precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0
    recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0
    f1_0 = 2 * precision_0 * recall_0 / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0
    
    precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_1 = 2 * precision_1 * recall_1 / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0
    
    accuracy = accuracy_score(y_test, y_pred)
    avg_precision = (precision_0 + precision_1) / 2
    avg_recall = (recall_0 + recall_1) / 2
    avg_f1 = (f1_0 + f1_1) / 2
    
    print("\n" + "="*80)
    print("ğŸ“š è¯„ä¼°æŒ‡æ ‡è¯¦ç»†è§£é‡Š - è®©ä¸æ‡‚ç»Ÿè®¡çš„äººä¹Ÿèƒ½çœ‹æ‡‚")
    print("="*80)
    
    print("\n" + "ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ".center(76))
    print("-" * 80)
    print("""
æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é¢„æµ‹ç³»ç»Ÿï¼Œç›®æ ‡æ˜¯é¢„æµ‹è‚¡ç¥¨æœªæ¥ä»·æ ¼ç›¸å¯¹äº20æ—¥å‡çº¿çš„ä½ç½®ï¼š
  â€¢ å¼ºåŠ¿ç±»åˆ«ï¼ˆæ ‡ç­¾0ï¼‰: é¢„æµ‹è‚¡ä»· â‰¥ MA20ï¼ˆçœ‹æ¶¨ä¿¡å·ï¼‰
  â€¢ å¼±åŠ¿ç±»åˆ«ï¼ˆæ ‡ç­¾1ï¼‰: é¢„æµ‹è‚¡ä»· < MA20ï¼ˆçœ‹è·Œä¿¡å·ï¼‰

æ¨¡å‹é€šè¿‡å­¦ä¹ å†å²æ•°æ®ä¸­çš„ç‰¹å¾æ¨¡å¼ï¼Œæ¥é¢„æµ‹æœªæ¥è‚¡ä»·çš„å¼ºå¼±çŠ¶æ€ã€‚
""")
    
    print("\n" + "ğŸ“Š æ··æ·†çŸ©é˜µè¯¦è§£".center(76))
    print("-" * 80)
    print(f"""
æ··æ·†çŸ©é˜µæ˜¯è¯„ä¼°åˆ†ç±»æ¨¡å‹çš„åŸºç¡€å·¥å…·ï¼Œå±•ç¤ºäº†é¢„æµ‹ç»“æœä¸çœŸå®æƒ…å†µçš„å¯¹æ¯”ï¼š

                    é¢„æµ‹ä¸ºå¼ºåŠ¿    é¢„æµ‹ä¸ºå¼±åŠ¿
    å®é™…ä¸ºå¼ºåŠ¿        {tn:6d}        {fp:6d}
    å®é™…ä¸ºå¼±åŠ¿        {fn:6d}        {tp:6d}

å››ä¸ªå…³é”®æ•°å­—çš„å«ä¹‰ï¼š

  âœ… çœŸé˜³æ€§(TP) = {tp:4d}: å®é™…å¼±åŠ¿ï¼Œé¢„æµ‹ä¹Ÿæ˜¯å¼±åŠ¿ â†’ é¢„æµ‹æ­£ç¡®ï¼
  âŒ å‡é˜³æ€§(FP) = {fp:4d}: å®é™…å¼ºåŠ¿ï¼Œä½†é¢„æµ‹æˆå¼±åŠ¿ â†’ è¯¯åˆ¤ä¸ºå¼±åŠ¿ï¼ˆé”™å¤±æœºä¼šï¼‰
  âŒ å‡é˜´æ€§(FN) = {fn:4d}: å®é™…å¼±åŠ¿ï¼Œä½†é¢„æµ‹æˆå¼ºåŠ¿ â†’ è¯¯åˆ¤ä¸ºå¼ºåŠ¿ï¼ˆå¯èƒ½è¸©é›·ï¼‰
  âœ… çœŸé˜´æ€§(TN) = {tn:4d}: å®é™…å¼ºåŠ¿ï¼Œé¢„æµ‹ä¹Ÿæ˜¯å¼ºåŠ¿ â†’ é¢„æµ‹æ­£ç¡®ï¼

æ€»é¢„æµ‹æ¬¡æ•° = {tn + fp + fn + tp} æ¬¡
""")
    
    print("\n" + "ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡è§£é‡Š".center(76))
    print("-" * 80)
    
    # 1. å‡†ç¡®ç‡
    print(f"""
1ï¸âƒ£  å‡†ç¡®ç‡ (Accuracy) = {accuracy:.2%}
    
    ğŸ’¡ é€šä¿—ç†è§£ï¼šåœ¨æ‰€æœ‰é¢„æµ‹ä¸­ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹æ˜¯çŒœå¯¹çš„ã€‚
    
    ğŸ“ è®¡ç®—å…¬å¼ï¼š(TP + TN) / (TP + TN + FP + FN)
                = ({tp} + {tn}) / {tn + fp + fn + tp}
                = {accuracy:.2%}
    
    ğŸ¯ å®é™…æ„ä¹‰ï¼š
       å¦‚æœå¯¹100åªè‚¡ç¥¨è¿›è¡Œé¢„æµ‹ï¼Œå¤§çº¦æœ‰ {accuracy*100:.0f} åªçš„é¢„æµ‹æ˜¯æ­£ç¡®çš„ã€‚
       
    âš–ï¸  è¯„ä»·æ ‡å‡†ï¼š
       â€¢ â‰¥ 80%: ä¼˜ç§€ â­â­â­â­â­
       â€¢ 70-80%: è‰¯å¥½ â­â­â­â­
       â€¢ 60-70%: ä¸­ç­‰ â­â­â­
       â€¢ < 60%: éœ€è¦æ”¹è¿› â­â­
""")
    
    # 2. ç²¾ç¡®ç‡
    print(f"""
2ï¸âƒ£  ç²¾ç¡®ç‡ (Precision) = {avg_precision:.2%}
    
    ğŸ’¡ é€šä¿—ç†è§£ï¼šåœ¨æ¨¡å‹é¢„æµ‹ä¸ºæŸä¸ªç±»åˆ«çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£å±äºè¯¥ç±»åˆ«çš„æ¯”ä¾‹ã€‚
    
    å¼ºåŠ¿ç±»åˆ«ç²¾ç¡®ç‡ = {precision_0:.2%}
    ğŸ“ è®¡ç®—ï¼šTN / (TN + FN) = {tn} / ({tn} + {fn}) = {precision_0:.2%}
    ğŸ¯ æ„ä¹‰ï¼šæ¨¡å‹è¯´"è¿™è‚¡ç¥¨ä¼šå¼ºåŠ¿"æ—¶ï¼Œæœ‰ {precision_0*100:.1f}% çš„æ¦‚ç‡æ˜¯å¯¹çš„ã€‚
    
    å¼±åŠ¿ç±»åˆ«ç²¾ç¡®ç‡ = {precision_1:.2%}
    ğŸ“ è®¡ç®—ï¼šTP / (TP + FP) = {tp} / ({tp} + {fp}) = {precision_1:.2%}
    ğŸ¯ æ„ä¹‰ï¼šæ¨¡å‹è¯´"è¿™è‚¡ç¥¨ä¼šå¼±åŠ¿"æ—¶ï¼Œæœ‰ {precision_1*100:.1f}% çš„æ¦‚ç‡æ˜¯å¯¹çš„ã€‚
    
    å¹³å‡ç²¾ç¡®ç‡ = {avg_precision:.2%}
    
    ğŸ¯ å®é™…åº”ç”¨ï¼š
       ç²¾ç¡®ç‡é«˜ = æ¨¡å‹è¯´çš„è¯å¯ä¿¡åº¦é«˜ = "å®ç¼ºæ¯‹æ»¥"
       å¦‚æœç²¾ç¡®ç‡æ˜¯75%ï¼Œæ„æ€æ˜¯æ¨¡å‹æ¨èçš„è‚¡ç¥¨ä¸­ï¼Œæœ‰75%çœŸçš„ç¬¦åˆé¢„æœŸã€‚
       
    âš–ï¸  è¯„ä»·æ ‡å‡†ï¼š
       â€¢ â‰¥ 75%: ä¼˜ç§€ï¼Œæ¨èå¯é  â­â­â­â­â­
       â€¢ 65-75%: è‰¯å¥½ï¼Œæœ‰å‚è€ƒä»·å€¼ â­â­â­â­
       â€¢ 55-65%: ä¸­ç­‰ï¼Œéœ€è°¨æ…å‚è€ƒ â­â­â­
       â€¢ < 55%: è¾ƒå·®ï¼Œä¸å»ºè®®ä½¿ç”¨ â­â­
""")
    
    # 3. å¬å›ç‡
    print(f"""
3ï¸âƒ£  å¬å›ç‡ (Recall) = {avg_recall:.2%}
    
    ğŸ’¡ é€šä¿—ç†è§£ï¼šåœ¨æ‰€æœ‰çœŸæ­£å±äºæŸç±»åˆ«çš„æ ·æœ¬ä¸­ï¼Œæ¨¡å‹æˆåŠŸè¯†åˆ«å‡ºçš„æ¯”ä¾‹ã€‚
    
    å¼ºåŠ¿ç±»åˆ«å¬å›ç‡ = {recall_0:.2%}
    ğŸ“ è®¡ç®—ï¼šTN / (TN + FP) = {tn} / ({tn} + {fp}) = {recall_0:.2%}
    ğŸ¯ æ„ä¹‰ï¼šåœ¨æ‰€æœ‰å®é™…å¼ºåŠ¿çš„è‚¡ç¥¨ä¸­ï¼Œæ¨¡å‹æˆåŠŸè¯†åˆ«å‡ºäº† {recall_0*100:.1f}%ã€‚
    
    å¼±åŠ¿ç±»åˆ«å¬å›ç‡ = {recall_1:.2%}
    ğŸ“ è®¡ç®—ï¼šTP / (TP + FN) = {tp} / ({tp} + {fn}) = {recall_1:.2%}
    ğŸ¯ æ„ä¹‰ï¼šåœ¨æ‰€æœ‰å®é™…å¼±åŠ¿çš„è‚¡ç¥¨ä¸­ï¼Œæ¨¡å‹æˆåŠŸè¯†åˆ«å‡ºäº† {recall_1*100:.1f}%ã€‚
    
    å¹³å‡å¬å›ç‡ = {avg_recall:.2%}
    
    ğŸ¯ å®é™…åº”ç”¨ï¼š
       å¬å›ç‡é«˜ = æ¨¡å‹"ä¸æ¼è¿‡"å¥½æœºä¼š = "å®å¯é”™æ€ï¼Œä¸å¯æ”¾è¿‡"
       å¦‚æœå¼ºåŠ¿å¬å›ç‡æ˜¯80%ï¼Œæ„æ€æ˜¯100åªçœŸæ­£å¼ºåŠ¿çš„è‚¡ç¥¨ä¸­ï¼Œæ¨¡å‹èƒ½æ‰¾å‡º80åªã€‚
       
    âš–ï¸  è¯„ä»·æ ‡å‡†ï¼š
       â€¢ â‰¥ 80%: ä¼˜ç§€ï¼Œè¦†ç›–å…¨é¢ â­â­â­â­â­
       â€¢ 70-80%: è‰¯å¥½ï¼ŒåŸºæœ¬è¦†ç›– â­â­â­â­
       â€¢ 60-70%: ä¸­ç­‰ï¼Œæœ‰é—æ¼ â­â­â­
       â€¢ < 60%: è¾ƒå·®ï¼Œé—æ¼è¾ƒå¤š â­â­
""")
    
    # 4. F1åˆ†æ•°
    print(f"""
4ï¸âƒ£  F1åˆ†æ•° (F1-Score) = {avg_f1:.2%}
    
    ğŸ’¡ é€šä¿—ç†è§£ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°ï¼Œç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
    
    å¼ºåŠ¿ç±»åˆ«F1 = {f1_0:.2%}
    ğŸ“ è®¡ç®—ï¼š2 Ã— (ç²¾ç¡®ç‡ Ã— å¬å›ç‡) / (ç²¾ç¡®ç‡ + å¬å›ç‡)
           = 2 Ã— ({precision_0:.2%} Ã— {recall_0:.2%}) / ({precision_0:.2%} + {recall_0:.2%})
           = {f1_0:.2%}
    
    å¼±åŠ¿ç±»åˆ«F1 = {f1_1:.2%}
    ğŸ“ è®¡ç®—ï¼š2 Ã— ({precision_1:.2%} Ã— {recall_1:.2%}) / ({precision_1:.2%} + {recall_1:.2%})
           = {f1_1:.2%}
    
    å¹³å‡F1åˆ†æ•° = {avg_f1:.2%}
    
    ğŸ¯ å®é™…æ„ä¹‰ï¼š
       F1åˆ†æ•°å¹³è¡¡äº†"å‡†ç¡®æ€§"å’Œ"è¦†ç›–é¢"ï¼Œæ˜¯æœ€é‡è¦çš„ç»¼åˆæŒ‡æ ‡ã€‚
       å½“ç²¾ç¡®ç‡å’Œå¬å›ç‡éƒ½é«˜æ—¶ï¼ŒF1åˆ†æ•°æ‰ä¼šé«˜ã€‚
       
    âš–ï¸  è¯„ä»·æ ‡å‡†ï¼š
       â€¢ â‰¥ 75%: ä¼˜ç§€ï¼Œæ¨¡å‹è¡¨ç°å¾ˆå¥½ â­â­â­â­â­
       â€¢ 65-75%: è‰¯å¥½ï¼Œæ¨¡å‹å¯ç”¨ â­â­â­â­
       â€¢ 55-65%: ä¸­ç­‰ï¼Œæœ‰æ”¹è¿›ç©ºé—´ â­â­â­
       â€¢ < 55%: è¾ƒå·®ï¼Œéœ€è¦ä¼˜åŒ– â­â­
""")
    
    print("\n" + "ğŸ“ å¦‚ä½•è§£è¯»è¿™äº›æŒ‡æ ‡ï¼Ÿ".center(76))
    print("-" * 80)
    print("""
ğŸ“Œ ç²¾ç¡®ç‡ vs å¬å›ç‡çš„æƒè¡¡ï¼š

  é«˜ç²¾ç¡®ç‡ã€ä½å¬å›ç‡ â†’ ä¿å®ˆç­–ç•¥
    ç‰¹ç‚¹ï¼šæ¨¡å‹æ¨èçš„å¾ˆå°‘ï¼Œä½†æ¨èçš„å¤§å¤šæ˜¯å¯¹çš„
    é€‚ç”¨ï¼šæƒ³è¦é«˜è´¨é‡æ¨èï¼Œå®æ„¿é”™è¿‡ä¹Ÿä¸æ„¿è¯¯åˆ¤
    ä¾‹å­ï¼šåªæ¨èæœ€æœ‰æŠŠæ¡çš„å¼ºåŠ¿è‚¡
  
  ä½ç²¾ç¡®ç‡ã€é«˜å¬å›ç‡ â†’ æ¿€è¿›ç­–ç•¥
    ç‰¹ç‚¹ï¼šæ¨¡å‹æ¨èçš„å¾ˆå¤šï¼Œä½†æœ‰ä¸å°‘æ˜¯é”™çš„
    é€‚ç”¨ï¼šä¸æƒ³é”™è¿‡ä»»ä½•æœºä¼šï¼Œå¯ä»¥æ¥å—ä¸€äº›è¯¯åˆ¤
    ä¾‹å­ï¼šæŠŠæ‰€æœ‰å¯èƒ½å¼ºåŠ¿çš„è‚¡ç¥¨éƒ½æ‰¾å‡ºæ¥
  
  é«˜ç²¾ç¡®ç‡ã€é«˜å¬å›ç‡ â†’ ç†æƒ³çŠ¶æ€ âœ¨
    ç‰¹ç‚¹ï¼šæ¨èçš„å¤šï¼Œè€Œä¸”å‡†ç¡®ç‡é«˜
    è¿™æ˜¯æˆ‘ä»¬è¿½æ±‚çš„ç›®æ ‡ï¼

ğŸ“Œ å®æˆ˜åº”ç”¨å»ºè®®ï¼š

  å¦‚æœæ‚¨æ˜¯ä¿å®ˆå‹æŠ•èµ„è€…ï¼š
    âœ… å…³æ³¨ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰> 70%
    âœ… é€‰æ‹©æ¨¡å‹é¢„æµ‹ä¸º"å¼ºåŠ¿"çš„è‚¡ç¥¨
    âœ… æ¥å—å¯èƒ½é”™è¿‡ä¸€äº›æœºä¼š
  
  å¦‚æœæ‚¨æ˜¯æ¿€è¿›å‹æŠ•èµ„è€…ï¼š
    âœ… å…³æ³¨å¬å›ç‡ï¼ˆRecallï¼‰> 70%
    âœ… å…³æ³¨æ¨¡å‹è¯†åˆ«å‡ºçš„æ‰€æœ‰å¯èƒ½æœºä¼š
    âœ… è‡ªå·±å†åšäºŒæ¬¡ç­›é€‰
  
  å¦‚æœæ‚¨è¿½æ±‚å¹³è¡¡ï¼š
    âœ… å…³æ³¨F1åˆ†æ•° > 70%
    âœ… ç»¼åˆè€ƒè™‘ç²¾ç¡®ç‡å’Œå¬å›ç‡
    âœ… è¿™æ˜¯æœ€æ¨èçš„ç­–ç•¥

ğŸ“Œ å½“å‰æ¨¡å‹è¯„ä»·ï¼š
""")
    
    if avg_precision >= 0.75 and avg_recall >= 0.70:
        print(f"""
  ğŸ‰ æ­å–œï¼æ‚¨çš„æ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼
     â€¢ å¹³å‡ç²¾ç¡®ç‡ {avg_precision:.2%} - æ¨èå¯é 
     â€¢ å¹³å‡å¬å›ç‡ {avg_recall:.2%} - è¦†ç›–å…¨é¢
     â€¢ å¹³å‡F1åˆ†æ•° {avg_f1:.2%} - ç»¼åˆä¼˜ç§€
     
  ğŸ’¡ å»ºè®®ï¼šå¯ä»¥å®ç›˜å°è§„æ¨¡æµ‹è¯•ï¼Œè§‚å¯Ÿå®é™…æ•ˆæœã€‚
""")
    elif avg_precision >= 0.65 and avg_recall >= 0.60:
        print(f"""
  ğŸ‘ ä¸é”™ï¼æ‚¨çš„æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼
     â€¢ å¹³å‡ç²¾ç¡®ç‡ {avg_precision:.2%} - æœ‰å‚è€ƒä»·å€¼
     â€¢ å¹³å‡å¬å›ç‡ {avg_recall:.2%} - åŸºæœ¬è¦†ç›–
     â€¢ å¹³å‡F1åˆ†æ•° {avg_f1:.2%} - ç»¼åˆè‰¯å¥½
     
  ğŸ’¡ å»ºè®®ï¼šå¯ä»¥ä½œä¸ºè¾…åŠ©å†³ç­–å·¥å…·ï¼Œç»“åˆå…¶ä»–æŒ‡æ ‡ä½¿ç”¨ã€‚
""")
    else:
        print(f"""
  âš ï¸  æ‚¨çš„æ¨¡å‹è¿˜æœ‰è¾ƒå¤§æå‡ç©ºé—´
     â€¢ å¹³å‡ç²¾ç¡®ç‡ {avg_precision:.2%}
     â€¢ å¹³å‡å¬å›ç‡ {avg_recall:.2%}
     â€¢ å¹³å‡F1åˆ†æ•° {avg_f1:.2%}
     
  ğŸ’¡ å»ºè®®ï¼š
     - å¢åŠ è®­ç»ƒæ•°æ®é‡
     - å°è¯•ä¸åŒçš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•
     - è°ƒæ•´æ¨¡å‹å‚æ•°
     - è€ƒè™‘ä½¿ç”¨é›†æˆå­¦ä¹ æ–¹æ³•
""")
    
    print("\n" + "="*80)
    print("ğŸ’¡ æ¸©é¦¨æç¤ºï¼š")
    print("   è‚¡ç¥¨æŠ•èµ„æœ‰é£é™©ï¼Œæ¨¡å‹é¢„æµ‹ä»…ä¾›å‚è€ƒã€‚")
    print("   è¯·ç»“åˆåŸºæœ¬é¢åˆ†æã€æŠ€æœ¯åˆ†æç­‰å¤šç§æ–¹æ³•ï¼Œè°¨æ…å†³ç­–ã€‚")
    print("   è¿‡å¾€è¡¨ç°ä¸ä»£è¡¨æœªæ¥æ”¶ç›Šã€‚")
    print("="*80 + "\n")

def main():
    """
    ç»Ÿè®¡åˆ†æä¸»å‡½æ•°
    """
    print("="*80)
    print("è‚¡ç¥¨ç»Ÿè®¡åˆ†æ")
    print("åŸºäºMA20çš„äºŒåˆ†ç±»é¢„æµ‹ï¼šä»·æ ¼>=MA20(å¼ºåŠ¿) vs ä»·æ ¼<MA20(å¼±åŠ¿)")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆ›å»ºç»“æœç›®å½•
    if not os.path.exists('results'):
        os.makedirs('results')
    
    try:
        # 1. åŠ è½½å¤„ç†åçš„ç‰¹å¾
        print("\næ­¥éª¤1: åŠ è½½å¤„ç†åçš„ç‰¹å¾")
        print("-" * 50)
        
        try:
            x_filtered, y_series = load_processed_features('data')
        except:
            print("é”™è¯¯: æ— æ³•åŠ è½½ç‰¹å¾æ•°æ®")
            print("è¯·å…ˆè¿è¡Œ: python stock_feature_engineering.py")
            return
        
        # 2. æ¸…ç†ç‰¹å¾åç§°ï¼ˆä¸æ¨¡å‹è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        print(f"\næ­¥éª¤2: æ¸…ç†ç‰¹å¾åç§°")
        print("-" * 50)
        print(f"æ¸…ç†å‰ç‰¹å¾ç¤ºä¾‹: {x_filtered.columns[:3].tolist()}")
        x_filtered = clean_feature_names(x_filtered)
        print(f"æ¸…ç†åç‰¹å¾ç¤ºä¾‹: {x_filtered.columns[:3].tolist()}")
        print(f"âœ… ç‰¹å¾åç§°æ¸…ç†å®Œæˆ")
        
        # 3. æ•°æ®åˆ†å‰²
        print(f"\næ­¥éª¤3: æ•°æ®åˆ†å‰²")
        print("-" * 50)
        X_train, X_test, y_train, y_test = train_test_split(
            x_filtered, y_series, 
            test_size=0.2, 
            random_state=42, 
            stratify=y_series
        )
        print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        # 4. æ¨¡å‹è®­ç»ƒ
        print(f"\næ­¥éª¤4: æ¨¡å‹è®­ç»ƒ")
        print("-" * 50)
        best_model, y_pred, accuracy, model_name, models_to_try = train_binary_model(
            X_train, X_test, y_train, y_test,
            use_smote=True,  # æ˜¯å¦ä½¿ç”¨SMOTEè¿‡é‡‡æ ·
            use_multi_models=True  # â­ æ”¹ä¸ºTrueï¼Œè®­ç»ƒå¤šä¸ªæ¨¡å‹ç”¨äºå¯¹æ¯”
        )
        
        # 5. ç»“æœå¯è§†åŒ–
        print(f"\næ­¥éª¤5: ç»“æœå¯è§†åŒ–")
        print("-" * 50)
        visualize_binary_results(y_test, y_pred, accuracy, output_dir='results')
        
        # 6. è¯¦ç»†æ€»ç»“
        print(f"\næ­¥éª¤6: ç”Ÿæˆè¯¦ç»†æ€»ç»“")
        print("-" * 50)
        
        # ä¼°ç®—è‚¡ç¥¨æ•°é‡ï¼ˆä»æ ·æœ¬IDä¸­æå–ï¼‰
        sample_ids = y_series.index.tolist()
        stock_codes = set([id.rsplit('_', 1)[0] for id in sample_ids])
        stock_count = len(stock_codes)
        
        print_detailed_summary(
            y_test, y_pred, accuracy, 
            stock_count=stock_count,
            feature_count=x_filtered.shape[1]
        )
        
        # 7. æ‰“å°è¯¦ç»†çš„æŒ‡æ ‡è§£é‡Š
        print(f"\næ­¥éª¤7: æ‰“å°è¯„ä¼°æŒ‡æ ‡è¯¦ç»†è§£é‡Š")
        print("-" * 50)
        print_metrics_explanation(y_test, y_pred)
        
        # 8. ä¿å­˜æ¨¡å‹å’Œç‰¹å¾åˆ—è¡¨ï¼ˆç”¨äºå®ç›˜é¢„æµ‹ï¼‰
        print(f"\næ­¥éª¤8: ä¿å­˜æ¨¡å‹ç”¨äºå®ç›˜é¢„æµ‹")
        print("-" * 50)
        
        import pickle
        
        os.makedirs('models', exist_ok=True)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
        model_save_path = 'models/trained_model.pkl'
        with open(model_save_path, 'wb') as f:
            pickle.dump(best_model, f)
        print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")
        
        # â­ æ–°å¢ï¼šä¿å­˜æ‰€æœ‰è®­ç»ƒçš„æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”é¢„æµ‹ï¼‰
        all_models_path = 'models/all_trained_models.pkl'
        all_models_data = {}
        
        for model_name_temp, model_temp in models_to_try.items():
            # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡
            y_proba_temp = model_temp.predict_proba(X_test)[:, 1]
            optimal_threshold_temp, _, _ = find_optimal_threshold(
                y_test, y_proba_temp, metric='precision', min_recall=0.3
            )
            y_pred_temp = (y_proba_temp >= optimal_threshold_temp).astype(int)
            
            precision_0 = precision_score(y_test, y_pred_temp, pos_label=0, zero_division=0)
            precision_1 = precision_score(y_test, y_pred_temp, pos_label=1, zero_division=0)
            avg_precision_temp = (precision_0 + precision_1) / 2
            accuracy_temp = accuracy_score(y_test, y_pred_temp)
            
            all_models_data[model_name_temp] = {
                'model': model_temp,
                'optimal_threshold': optimal_threshold_temp,
                'accuracy': accuracy_temp,
                'avg_precision': avg_precision_temp,
                'precision_0': precision_0,
                'precision_1': precision_1
            }
        
        with open(all_models_path, 'wb') as f:
            pickle.dump(all_models_data, f)
        print(f"âœ… æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ°: {all_models_path} (å…±{len(all_models_data)}ä¸ªæ¨¡å‹)")
        
        # ä¿å­˜ç‰¹å¾åˆ—è¡¨ï¼ˆç”¨äºé¢„æµ‹æ—¶å¯¹é½ç‰¹å¾ï¼‰
        feature_list_path = 'models/feature_list.pkl'
        with open(feature_list_path, 'wb') as f:
            pickle.dump(x_filtered.columns.tolist(), f)
        print(f"âœ… ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜åˆ°: {feature_list_path}")
        
        # ä¿å­˜æ¨¡å‹å…ƒä¿¡æ¯
        model_info = {
            'best_model_name': model_name,
            'train_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'accuracy': accuracy,
            'avg_precision': (precision_score(y_test, y_pred, pos_label=0, zero_division=0) + 
                             precision_score(y_test, y_pred, pos_label=1, zero_division=0)) / 2,
            'feature_count': len(x_filtered.columns),
            'sample_count': len(y_series),
            'stock_count': stock_count,
            'available_models': list(all_models_data.keys())
        }
        
        model_info_path = 'models/model_info.pkl'
        with open(model_info_path, 'wb') as f:
            pickle.dump(model_info, f)
        print(f"âœ… æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜åˆ°: {model_info_path}")
        
        print(f"\nå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("\nâœ… ç»Ÿè®¡åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° results/ ç›®å½•")
        print("âœ… æ¨¡å‹å·²ä¿å­˜ï¼Œå¯ä½¿ç”¨ stock_live_prediction.py è¿›è¡Œå®ç›˜é¢„æµ‹")
        
    except Exception as e:
        print(f"ç»Ÿè®¡åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()


