"""
è‚¡ç¥¨ç‰¹å¾å·¥ç¨‹æ¨¡å—
è´Ÿè´£TSFreshç‰¹å¾æå–ã€ç‰¹å¾é€‰æ‹©ç­‰æ•°æ®å¤„ç†å·¥ä½œ
"""

import pandas as pd
import numpy as np
import pickle
import os
import warnings
from tsfresh import extract_features, select_features
from tsfresh.utilities.dataframe_functions import impute
from sklearn.ensemble import RandomForestClassifier
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# æŠ‘åˆ¶joblib/lokyåœ¨Windowsä¸Šçš„CPUæ ¸å¿ƒæ•°è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='joblib')
os.environ['LOKY_MAX_CPU_COUNT'] = str(mp.cpu_count())

def load_stock_data(data_path='data/all_stock_data.pkl'):
    """
    ä»æ–‡ä»¶åŠ è½½è‚¡ç¥¨æ•°æ®
    
    å‚æ•°:
    data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    all_data: è‚¡ç¥¨æ•°æ®å­—å…¸
    """
    print(f"æ­£åœ¨åŠ è½½è‚¡ç¥¨æ•°æ®...")
    print(f"æ•°æ®è·¯å¾„: {data_path}")
    
    if not os.path.exists(data_path):
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ - {data_path}")
        print("è¯·å…ˆè¿è¡Œ stock_data_downloader.py ä¸‹è½½æ•°æ®")
        return None
    
    try:
        with open(data_path, 'rb') as f:
            all_data = pickle.load(f)
        
        print(f"æˆåŠŸåŠ è½½ {len(all_data)} åªè‚¡ç¥¨çš„æ•°æ®")
        for stock_code, data in all_data.items():
            print(f"  {stock_code}: {len(data)} æ¡è®°å½•, {data.shape[1]} ä¸ªç‰¹å¾")
        
        return all_data
    
    except Exception as e:
        print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return None

def create_multi_feature_tsfresh_data(all_data, window_size=30, forecast_horizon=5):
    """
    åˆ›å»ºå¤šç‰¹å¾TSFreshæ ¼å¼æ•°æ®
    
    å‚æ•°:
    all_data: è‚¡ç¥¨æ•°æ®å­—å…¸
    window_size: æ»‘åŠ¨çª—å£å¤§å°
    forecast_horizon: é¢„æµ‹æœŸé™
    
    è¿”å›:
    x_df: TSFreshæ ¼å¼çš„ç‰¹å¾æ•°æ®
    y_df: ç›®æ ‡å˜é‡æ•°æ®
    """
    print(f"æ­£åœ¨è½¬æ¢å¤šç‰¹å¾æ•°æ®ä¸ºTSFreshæ ¼å¼...")
    print(f"çª—å£å¤§å°: {window_size}å¤©")
    print(f"é¢„æµ‹æœŸé™: {forecast_horizon}å¤©")
    
    tsfresh_data_list = []
    target_list = []
    
    # é€‰æ‹©ç”¨äºTSFreshåˆ†æçš„ç‰¹å¾
    tsfresh_features = [
        'Close', 'Open', 'High', 'Low', 'Volume', 'TurnoverRate', 
        'PriceChangeRate', 'MainNetInflow', 'MainNetInflowRatio'
    ]

    for stock_code, data in all_data.items():
        print(f"å¤„ç†è‚¡ç¥¨ {stock_code}...")
        
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„åˆ—éƒ½å­˜åœ¨
        for feature in tsfresh_features:
            if feature not in data.columns:
                data[feature] = 0
                
        # æ£€æŸ¥æ•°æ®é•¿åº¦
        if len(data) < window_size + forecast_horizon:
            print(f"è­¦å‘Š: {stock_code} æ•°æ®ç‚¹ä¸è¶³ï¼Œè·³è¿‡")
            continue

        # ä¸ºæ¯ä¸ªæ»‘åŠ¨çª—å£åˆ›å»ºæ ·æœ¬
        for i in range(window_size, len(data) - forecast_horizon):
            window_id = f"{stock_code}_{i}"
            
            # æå–å¤šä¸ªç‰¹å¾çš„çª—å£æ•°æ®
            for feature in tsfresh_features:
                feature_window = data[feature].iloc[i - window_size : i]
                
                # åˆ›å»ºtsfreshæ ¼å¼çš„DataFrameç‰‡æ®µ
                for time_idx, value in enumerate(feature_window.values):
                    tsfresh_data_list.append({
                        'id': window_id,
                        'time': time_idx,
                        'feature_name': feature,
                        'value': float(value) if not pd.isna(value) else 0.0
                    })

            # åˆ›å»ºäºŒåˆ†ç±»ç›®æ ‡å˜é‡ï¼ˆåŸºäºMA20ï¼‰
            future_close = float(data['Close'].iloc[i + forecast_horizon])
            
            # è·å–æœªæ¥æ—¶ç‚¹çš„MA20
            if 'MA_20' in data.columns:
                future_ma20 = float(data['MA_20'].iloc[i + forecast_horizon])
            elif 'SMA_20' in data.columns:
                future_ma20 = float(data['SMA_20'].iloc[i + forecast_horizon])
            else:
                # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„MA20ï¼Œä¸´æ—¶è®¡ç®—
                future_ma20 = float(data['Close'].iloc[i + forecast_horizon - 19 : i + forecast_horizon + 1].mean())
            
            # äºŒåˆ†ç±»æ ‡ç­¾ï¼š0=æ”¶ç›˜ä»·>=MA20(å¼ºåŠ¿), 1=æ”¶ç›˜ä»·<MA20(å¼±åŠ¿)
            target = 1 if future_close < future_ma20 else 0
            target_list.append({'id': window_id, 'target': target})

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    x_df = pd.DataFrame(tsfresh_data_list)
    y_df = pd.DataFrame(target_list)
    
    print(f"ç”Ÿæˆçš„ç‰¹å¾æ•°æ®å½¢çŠ¶: {x_df.shape}")
    print(f"ç”Ÿæˆçš„ç›®æ ‡æ•°æ®å½¢çŠ¶: {y_df.shape}")
    print(f"åŒ…å«çš„ç‰¹å¾ç±»å‹: {x_df['feature_name'].unique()}")

    return x_df, y_df

def get_optimal_thread_count():
    """
    è·å–æœ€ä¼˜çº¿ç¨‹æ•°é…ç½®
    
    è¿”å›:
    max_workers: æœ€ä¼˜çº¿ç¨‹æ•°
    """
    cpu_count = mp.cpu_count()
    
    # æ ¹æ®CPUæ ¸å¿ƒæ•°æ™ºèƒ½é…ç½®çº¿ç¨‹æ•°
    if cpu_count <= 2:
        return 1  # åŒæ ¸åŠä»¥ä¸‹ä½¿ç”¨å•çº¿ç¨‹
    elif cpu_count <= 4:
        return 2  # å››æ ¸ä½¿ç”¨2çº¿ç¨‹
    elif cpu_count <= 8:
        return 4  # å…«æ ¸ä½¿ç”¨4çº¿ç¨‹
    else:
        return min(6, cpu_count // 2)  # é«˜æ ¸å¿ƒæ•°é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°

def advanced_feature_selection(x_extracted, y_series, method='hybrid'):
    """
    é«˜çº§ç‰¹å¾é€‰æ‹©æ–¹æ³•
    
    å‚æ•°:
    x_extracted: æå–çš„åŸå§‹ç‰¹å¾
    y_series: ç›®æ ‡å˜é‡
    method: ç‰¹å¾é€‰æ‹©æ–¹æ³• ('statistical', 'importance', 'hybrid', 'recursive')
    
    è¿”å›:
    x_filtered: ç­›é€‰åçš„ç‰¹å¾
    """
    print(f"\nğŸ” æ‰§è¡Œé«˜çº§ç‰¹å¾é€‰æ‹© (æ–¹æ³•: {method})...")
    
    if method == 'statistical':
        # æ–¹æ³•1: ç»Ÿè®¡æ˜¾è‘—æ€§ç­›é€‰
        from tsfresh.feature_selection.relevance import calculate_relevance_table
        relevance_table = calculate_relevance_table(x_extracted, y_series)
        relevant_features = relevance_table[relevance_table.relevant].feature
        x_filtered = x_extracted[relevant_features]
        print(f"  ç»Ÿè®¡ç­›é€‰åç‰¹å¾æ•°: {len(relevant_features)}")
        
    elif method == 'importance':
        # æ–¹æ³•2: åŸºäºç‰¹å¾é‡è¦æ€§çš„ç­›é€‰
        print("  è®­ç»ƒåˆæ­¥æ¨¡å‹è¯„ä¼°ç‰¹å¾é‡è¦æ€§...")
        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        rf.fit(x_extracted, y_series)
        
        importances = pd.DataFrame({
            'feature': x_extracted.columns,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        # ä¿ç•™é‡è¦æ€§å‰50%çš„ç‰¹å¾
        threshold = importances['importance'].quantile(0.5)
        selected_features = importances[importances['importance'] >= threshold]['feature'].tolist()
        x_filtered = x_extracted[selected_features]
        print(f"  é‡è¦æ€§ç­›é€‰åç‰¹å¾æ•°: {len(selected_features)}")
        
    elif method == 'recursive':
        # æ–¹æ³•3: é€’å½’ç‰¹å¾æ¶ˆé™¤
        from sklearn.feature_selection import RFECV
        print("  æ‰§è¡Œé€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰...")
        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        rfecv = RFECV(estimator=rf, step=10, cv=3, scoring='precision_macro', n_jobs=-1)
        rfecv.fit(x_extracted, y_series)
        x_filtered = x_extracted.iloc[:, rfecv.support_]
        print(f"  é€’å½’æ¶ˆé™¤åç‰¹å¾æ•°: {x_filtered.shape[1]}")
        
    else:  # hybrid
        # æ–¹æ³•4: æ··åˆç­–ç•¥ï¼ˆæ¨èï¼‰
        print("  ç¬¬ä¸€é˜¶æ®µ: ç»Ÿè®¡ç­›é€‰...")
        try:
            x_filtered = select_features(x_extracted, y_series, fdr_level=0.01)
            print(f"    ç»Ÿè®¡ç­›é€‰ä¿ç•™: {x_filtered.shape[1]} ä¸ªç‰¹å¾")
        except:
            print("    ç»Ÿè®¡ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾")
            x_filtered = x_extracted
        
        if x_filtered.shape[1] > 100:
            print("  ç¬¬äºŒé˜¶æ®µ: é‡è¦æ€§ç­›é€‰...")
            rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
            rf.fit(x_filtered, y_series)
            
            importances = pd.DataFrame({
                'feature': x_filtered.columns,
                'importance': rf.feature_importances_
            }).sort_values('importance', ascending=False)
            
            # ä¿ç•™ç´¯è®¡é‡è¦æ€§è¾¾åˆ°95%çš„ç‰¹å¾
            cumsum = importances['importance'].cumsum()
            n_features = (cumsum <= 0.95).sum() + 1
            selected_features = importances.head(n_features)['feature'].tolist()
            x_filtered = x_filtered[selected_features]
            print(f"    é‡è¦æ€§ç­›é€‰ä¿ç•™: {len(selected_features)} ä¸ªç‰¹å¾ï¼ˆç´¯è®¡é‡è¦æ€§95%ï¼‰")
    
    return x_filtered

def extract_enhanced_features(x_df, y_df, use_minimal=True):
    """
    æå–å¢å¼ºçš„TSFreshç‰¹å¾
    
    å‚æ•°:
    x_df: TSFreshæ ¼å¼çš„è¾“å…¥æ•°æ®
    y_df: ç›®æ ‡å˜é‡æ•°æ®
    use_minimal: æ˜¯å¦ä½¿ç”¨ç²¾ç®€çš„ç‰¹å¾é›†ï¼ˆæ¨èTrueï¼Œé€Ÿåº¦å¿«ï¼‰
    
    è¿”å›:
    x_filtered: ç­›é€‰åçš„ç‰¹å¾
    y_series: ç›®æ ‡å˜é‡Series
    """
    print("å¼€å§‹æå–å¢å¼ºTSFreshç‰¹å¾...")
    
    # ç‰¹å¾æå–è®¾ç½®
    if use_minimal:
        print("ä½¿ç”¨ç²¾ç®€ç‰¹å¾é›†ï¼ˆé€Ÿåº¦å¿«ï¼Œæ¨èï¼‰")
        from tsfresh.feature_extraction import MinimalFCParameters
        feature_extraction_settings = MinimalFCParameters()
    else:
        print("ä½¿ç”¨å®Œæ•´ç‰¹å¾é›†ï¼ˆç‰¹å¾å¤šï¼Œä½†è€—æ—¶é•¿ï¼‰")
        from tsfresh.feature_extraction import ComprehensiveFCParameters
        feature_extraction_settings = ComprehensiveFCParameters()
    
    # æ™ºèƒ½å¹¶è¡Œå¤„ç†é…ç½®
    cpu_count = mp.cpu_count()
    max_workers = get_optimal_thread_count()
    
    print(f"ç³»ç»ŸCPUæ ¸å¿ƒæ•°: {cpu_count}")
    print(f"ä½¿ç”¨å¤šçº¿ç¨‹æ¨¡å¼ï¼Œæœ€ä¼˜çº¿ç¨‹æ•°: {max_workers}")
    
    # ä¸ºæ¯ä¸ªç‰¹å¾ç±»å‹å•ç‹¬è¿›è¡Œç‰¹å¾æå–
    feature_types = x_df['feature_name'].unique()
    all_extracted_features = []
    
    def extract_single_feature_type(feature_type):
        """æå–å•ä¸ªç‰¹å¾ç±»å‹çš„TSFreshç‰¹å¾ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        try:
            print(f"å¼€å§‹æå– {feature_type} ç‰¹å¾...")
            
            feature_df = x_df[x_df['feature_name'] == feature_type].copy()
            feature_df = feature_df.drop('feature_name', axis=1)
            
            extracted = extract_features(
                feature_df,
                column_id="id",
                column_sort="time",
                column_value="value",
                default_fc_parameters=feature_extraction_settings,
                n_jobs=1,  # æ¯ä¸ªçº¿ç¨‹å†…éƒ¨ä½¿ç”¨å•è¿›ç¨‹
                disable_progressbar=True
            )
            
            # é‡å‘½ååˆ—ä»¥åŒ…å«ç‰¹å¾ç±»å‹
            extracted.columns = [f"{feature_type}_{col}" for col in extracted.columns]
            
            print(f"âœ… {feature_type} ç‰¹å¾æå–å®Œæˆï¼Œç‰¹å¾æ•°: {extracted.shape[1]}")
            return extracted
            
        except Exception as e:
            print(f"âŒ {feature_type} ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œæå–
    print(f"å¼€å§‹å¹¶è¡Œæå– {len(feature_types)} ç§ç‰¹å¾ç±»å‹...")
    
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_feature = {
            executor.submit(extract_single_feature_type, feature_type): feature_type 
            for feature_type in feature_types
        }
        
        completed_count = 0
        total_features = 0
        
        for future in as_completed(future_to_feature):
            feature_type = future_to_feature[future]
            try:
                extracted = future.result()
                if extracted is not None:
                    all_extracted_features.append(extracted)
                    total_features += extracted.shape[1]
                    
                completed_count += 1
                elapsed_time = time.time() - start_time
                print(f"è¿›åº¦: {completed_count}/{len(feature_types)} å®Œæˆï¼Œ"
                      f"å·²æå–ç‰¹å¾æ•°: {total_features}ï¼Œ"
                      f"è€—æ—¶: {elapsed_time:.1f}ç§’")
                      
            except Exception as e:
                print(f"çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸ {feature_type}: {e}")
                completed_count += 1
    
    total_time = time.time() - start_time
    print(f"å¤šçº¿ç¨‹ç‰¹å¾æå–å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"å¹³å‡æ¯ç§ç‰¹å¾ç±»å‹è€—æ—¶: {total_time/len(feature_types):.2f}ç§’")
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    if all_extracted_features:
        x_extracted = pd.concat(all_extracted_features, axis=1)
    else:
        print("é”™è¯¯: æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾")
        return None, None
    
    print(f"æå–çš„åŸå§‹ç‰¹å¾æ•°é‡: {x_extracted.shape[1]}")
    
    # å¤„ç†æ— é™å€¼å’Œç¼ºå¤±å€¼
    print("å¤„ç†ç¼ºå¤±å€¼å’Œæ— é™å€¼...")
    x_extracted = impute(x_extracted)
    
    # å‡†å¤‡ç›®æ ‡å˜é‡
    y_series = y_df.set_index('id')['target']
    x_extracted = x_extracted.loc[y_series.index]
    
    # ä½¿ç”¨é«˜çº§ç‰¹å¾é€‰æ‹©
    print("\n" + "="*70)
    print("ğŸš€ V2.4 ç²¾ç¡®ç‡æå‡ - æ™ºèƒ½ç‰¹å¾é€‰æ‹©")
    print("="*70)
    x_filtered = advanced_feature_selection(x_extracted, y_series, method='hybrid')
    print(f"âœ… æœ€ç»ˆç‰¹å¾æ•°é‡: {x_filtered.shape[1]}")
    print(f"   ç‰¹å¾ç­›é€‰æ¯”ç‡: {x_filtered.shape[1]/x_extracted.shape[1]:.2%}")
    
    return x_filtered, y_series

def save_processed_features(x_filtered, y_series, output_dir='data'):
    """
    ä¿å­˜å¤„ç†åçš„ç‰¹å¾æ•°æ®
    
    å‚æ•°:
    x_filtered: ç­›é€‰åçš„ç‰¹å¾
    y_series: ç›®æ ‡å˜é‡
    output_dir: è¾“å‡ºç›®å½•
    """
    print(f"\nä¿å­˜å¤„ç†åçš„ç‰¹å¾æ•°æ®...")
    
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # ä¿å­˜ä¸ºCSV
    x_filtered.to_csv(f'{output_dir}/processed_features.csv')
    y_series.to_csv(f'{output_dir}/processed_targets.csv')
    
    # ä¿å­˜ä¸ºpickleï¼ˆæ›´å¿«ï¼‰
    with open(f'{output_dir}/processed_features.pkl', 'wb') as f:
        pickle.dump({'X': x_filtered, 'y': y_series}, f)
    
    print(f"âœ… ç‰¹å¾æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}/")
    print(f"   - processed_features.csv ({x_filtered.shape[1]} ä¸ªç‰¹å¾)")
    print(f"   - processed_targets.csv ({len(y_series)} ä¸ªæ ·æœ¬)")
    print(f"   - processed_features.pkl (pickleæ ¼å¼ï¼ŒåŠ è½½æ›´å¿«)")

def load_processed_features(input_dir='data'):
    """
    åŠ è½½å¤„ç†åçš„ç‰¹å¾æ•°æ®
    
    å‚æ•°:
    input_dir: è¾“å…¥ç›®å½•
    
    è¿”å›:
    x_filtered: ç‰¹å¾æ•°æ®
    y_series: ç›®æ ‡å˜é‡
    """
    print(f"æ­£åœ¨åŠ è½½å¤„ç†åçš„ç‰¹å¾æ•°æ®...")
    
    pkl_path = f'{input_dir}/processed_features.pkl'
    
    if os.path.exists(pkl_path):
        print(f"ä»pickleæ–‡ä»¶åŠ è½½ï¼ˆæ›´å¿«ï¼‰...")
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
        x_filtered = data['X']
        y_series = data['y']
        print(f"âœ… åŠ è½½å®Œæˆ: {x_filtered.shape[1]} ä¸ªç‰¹å¾, {len(y_series)} ä¸ªæ ·æœ¬")
        return x_filtered, y_series
    else:
        print(f"ä»CSVæ–‡ä»¶åŠ è½½...")
        x_filtered = pd.read_csv(f'{input_dir}/processed_features.csv', index_col=0)
        y_series = pd.read_csv(f'{input_dir}/processed_targets.csv', index_col=0, squeeze=True)
        print(f"âœ… åŠ è½½å®Œæˆ: {x_filtered.shape[1]} ä¸ªç‰¹å¾, {len(y_series)} ä¸ªæ ·æœ¬")
        return x_filtered, y_series

def main():
    """
    ç‰¹å¾å·¥ç¨‹ä¸»å‡½æ•°
    """
    from datetime import datetime
    
    print("="*80)
    print("è‚¡ç¥¨ç‰¹å¾å·¥ç¨‹å¤„ç†")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # 1. åŠ è½½è‚¡ç¥¨æ•°æ®
        print("\næ­¥éª¤1: åŠ è½½è‚¡ç¥¨æ•°æ®")
        print("-" * 50)
        stock_data = load_stock_data('data/all_stock_data.pkl')
        
        if not stock_data:
            print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            print("è¯·å…ˆè¿è¡Œ: python stock_data_downloader.py")
            return
        
        # 2. è½¬æ¢ä¸ºTSFreshæ ¼å¼
        print(f"\næ­¥éª¤2: è½¬æ¢ä¸ºTSFreshæ ¼å¼")
        print("-" * 50)
        x_df, y_df = create_multi_feature_tsfresh_data(
            stock_data, 
            window_size=20,  # å¯è°ƒæ•´ï¼šè§‚å¯Ÿçª—å£å¤§å°
            forecast_horizon=5  # å¯è°ƒæ•´ï¼šé¢„æµ‹æœŸé™
        )
        
        if x_df.empty or y_df.empty:
            print("æ•°æ®è½¬æ¢å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # 3. ç‰¹å¾æå–å’Œé€‰æ‹©
        print(f"\næ­¥éª¤3: ç‰¹å¾æå–å’Œé€‰æ‹©")
        print("-" * 50)
        x_filtered, y_series = extract_enhanced_features(
            x_df, y_df, 
            use_minimal=True  # True=å¿«é€Ÿæ¨¡å¼, False=å®Œæ•´ç‰¹å¾
        )
        
        if x_filtered is None:
            print("ç‰¹å¾æå–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # 4. ä¿å­˜å¤„ç†åçš„ç‰¹å¾
        print(f"\næ­¥éª¤4: ä¿å­˜å¤„ç†åçš„ç‰¹å¾")
        print("-" * 50)
        save_processed_features(x_filtered, y_series, output_dir='data')
        
        # 5. æ€»ç»“
        print("\n" + "="*80)
        print("ç‰¹å¾å·¥ç¨‹å®Œæˆï¼")
        print("="*80)
        print(f"å¤„ç†è‚¡ç¥¨æ•°é‡: {len(stock_data)}")
        print(f"ç”Ÿæˆæ ·æœ¬æ•°é‡: {len(y_series)}")
        print(f"æå–ç‰¹å¾æ•°é‡: {x_filtered.shape[1]}")
        
        class_dist = pd.Series(y_series).value_counts().sort_index()
        print(f"\nç›®æ ‡å˜é‡åˆ†å¸ƒ:")
        print(f"  å¼ºåŠ¿(>=MA20): {class_dist.get(0, 0)} æ ·æœ¬ ({class_dist.get(0, 0)/len(y_series):.2%})")
        print(f"  å¼±åŠ¿(<MA20): {class_dist.get(1, 0)} æ ·æœ¬ ({class_dist.get(1, 0)/len(y_series):.2%})")
        
        print(f"\nå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("\nä¸‹ä¸€æ­¥: è¿è¡Œ stock_statistical_analysis.py è¿›è¡Œç»Ÿè®¡åˆ†æ")
        
    except Exception as e:
        print(f"ç‰¹å¾å·¥ç¨‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()


